Site Reliability Engineering: How Google Runs Production Systems
Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy
Last accessed on Monday August 7, 2017
501 Highlight(s)
                
Location: 158
SRE Way in mind: thoroughness and dedication, belief in the value of preparation and documentation, and an awareness of what could go wrong, coupled with a strong desire to prevent it.
                

Location: 288
Ben Treynor Sloss, the senior VP overseeing technical operations at Google — and the originator of the term “Site Reliability Engineering” —
                

Location: 313
Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.
                

Location: 321
At their core, the development teams want to launch new features and see them adopted by users. At their core, the ops teams want to make sure the service doesn’t break while they are holding the pager.
                

Location: 323
Because most outages are caused by some kind of change — a new configuration, a new feature launch, or a new type of user traffic — the two teams’ goals are fundamentally in tension.
                

Location: 335
Site Reliability Engineering teams focus on hiring software engineers to run our products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.
                

Location: 338
SRE is what happens when you ask a software engineer to design an operations team.
                

Location: 352
The result of our approach to hiring for SRE is that we end up with a team of people who (a) will quickly become bored by performing tasks by hand, and (b) have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.
                

Location: 358
Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload.
                

Pink highlightLocation: 360
To avoid this fate, the team tasked with managing a service needs to code or it will drown. Therefore, Google places a 50% cap on the aggregate “ops” work for all SREs — tickets, on-call, manual tasks, etc. This cap ensures that the SRE team has enough time in their schedule to make the service stable and operable.
                

Location: 364
we want systems that are automatic, not just automated.
                

Location: 366
Google’s rule of thumb is that an SRE team must spend the remaining 50% of its time actually doing development.
                

Location: 370
creative, autonomous engineering,
                

Location: 376
the number of SREs needed to run, maintain, and improve a system scales sublinearly with the size of the system.
                

Location: 393
Tenets of SRE
                

Location: 396
In general, an SRE team is responsible for the availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their service(s).
                

Location: 402
Google caps operational work for SREs at 50% of their time. Their remaining time should be spent using their coding skills on project work.
                

Location: 403
redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, [re]integrating developers into on-call pager rotations, and so on. The redirection ends when the operational load drops back to 50% or lower. This also provides an effective feedback mechanism, guiding developers to build systems that don’t need manual intervention.
                

Location: 409
When they are focused on operations work, on average, SREs should receive a maximum of two events per 8–12-hour on-call shift.
                

Location: 414
Postmortems should be written for all significant incidents, regardless of whether or not they paged; postmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps.
                

Location: 416
Google operates under a blame-free postmortem culture, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them.
                

Location: 424
100% is the wrong reliability target for basically everything (pacemakers and anti-lock brakes being notable exceptions).
                

Pink highlightLocation: 427
the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.
                

Location: 439
SRE’s goal is no longer “zero outages”; rather, SREs and product developers aim to spend the error budget getting maximum feature velocity.
                

Pink highlightLocation: 447
Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.
                

Location: 449
There are three kinds of valid monitoring output:
                

Location: 450
Alerts
                

Location: 452
Tickets
                

Location: 454
Logging
                

Location: 455
The expectation is that no one reads logs unless something else prompts them to do so.
                

Location: 459
Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR)
                

Location: 463
When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a “playbook” produces roughly a 3x improvement in MTTR as compared to the strategy of “winging it.”
                

Pink highlightLocation: 467
on-call playbooks,
                

Pink highlightLocation: 472
SRE has found that roughly 70% of outages are due to changes in a live system.
                

Location: 478
Demand forecasting and capacity planning can be viewed as ensuring that there is sufficient capacity and redundancy to serve projected future demand with the required availability.
                

Location: 483
Several steps are mandatory in capacity planning: An accurate organic demand forecast, which extends beyond the lead time required for acquiring capacity An accurate incorporation of inorganic demand sources into the demand forecast Regular load testing of the system to correlate raw capacity (servers, disks, and so on) to service capacity
                

Location: 497
Resource use is a function of demand (load), capacity, and software efficiency.
                

Location: 500
Software systems become slower as load is added to them. A slowdown in a service equates to a loss of capacity.
                

Location: 507
— it has become much more: a set of principles, a set of practices, a set of incentives, and a field of endeavor within the larger software engineering discipline.
                

Location: 576
Because tasks are fluidly allocated over machines, we can’t simply rely on IP addresses and port numbers to refer to the tasks. We solve this problem with an extra level of indirection: when starting a job, Borg allocates a name and index number to each task using the Borg Naming Service (BNS).
                

Location: 601
Bigtable supports eventually consistent, cross-datacenter replication.
                

Location: 609
OpenFlow-based software-defined network.
                

Location: 612
Bandwidth Enforcer
                

Location: 617
Global Software Load Balancer (GSLB)
                

Location: 627
asynchronous Consensus
                

Location: 631
Monitoring and Alerting
                

Location: 634
Borgmon monitoring program
                

Location: 669
Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.
                

Location: 768
Site Reliability Engineering seeks to balance the risk of unavailability with the goals of rapid innovation and efficient service operations,
                

Location: 780
In SRE, we manage service reliability largely by managing risk. We conceptualize risk as a continuum. We give equal importance to figuring out how to engineer greater reliability into Google systems and identifying the appropriate level of tolerance for the services we run.
                

Location: 783
We strive to make a service reliable enough, but no more reliable than it needs to be.
                

Location: 794
unplanned downtime
                

Location: 799
Time-based availability
                

Location: 800
Sorry, we’re unable to display this type of content.
        

Location: 808
instead of using metrics around uptime, we define availability in terms of the request success rate
                

Location: 810
Sorry, we’re unable to display this type of content.
        

Location: 813
In a typical application, not all requests are equal: failing a new user sign-up request is different from failing a request polling for new email in the background. In many cases, however, availability calculated as the request success rate over all requests is a reasonable approximation of unplanned downtime, as viewed from the end-user perspective.
                

Location: 816
Most nonserving systems (e.g., batch, pipeline, storage, and transactional systems) have a well-defined notion of successful and unsuccessful units of work.
                

Location: 820
request success rate
                

Location: 821
we set quarterly availability targets for a service and track our performance against those targets on a weekly, or even daily, basis.
                

Location: 838
There are many factors to consider when assessing the risk tolerance of services, such as the following: What level of availability is required? Do different types of failures have different effects on the service? How can we use the service cost to help locate a service on the risk continuum? What other service metrics are important to take into account? Target level of availability The target level of availability for a given Google service usually depends on the function it provides and how the service is positioned in the marketplace. The following list includes issues to consider: What level of service will the users expect? Does this service tie directly to revenue (either our revenue, or our customers’ revenue)? Is this a paid service, or is it free? If there are competitors in the marketplace, what level of service do those competitors provide? Is this service targeted at consumers, or at enterprises?
                

Location: 926
The key strategy with regards to infrastructure is to deliver services with explicitly delineated levels of service, thus enabling the clients to make the right risk and cost trade-offs when building their systems.
                

Location: 962
Canary duration and size
                

Location: 963
It’s a best practice to test a new release on some small subset of a typical workload, a practice often called canarying. How long do we wait, and how big is the canary?
                

Location: 969
Error Budget
                

Location: 983
Many products use this control loop to manage release velocity: as long as the system’s SLOs are met, releases can continue. If SLO violations occur frequently enough to expend the error budget, releases are temporarily halted while additional resources are invested in system testing and development to make the system more resilient, improve its performance, and so
                

Location: 997
Key Insights Managing service reliability is largely about managing risk, and managing risk can be costly. 100% is probably never the right reliability target: not only is it impossible to achieve, it’s typically more reliability than a service’s users want or notice. Match the profile of the service to the risk the business is willing to take. An error budget aligns incentives and emphasizes joint ownership between SRE and product development. Error budgets make it easier to decide the rate of releases and to effectively defuse discussions about outages with stakeholders, and allows multiple teams to reach the same conclusion about production risk without rancor.
                

Location: 1,012
We use intuition, experience, and an understanding of what users want to define service level indicators (SLIs), objectives (SLOs), and agreements (SLAs). These measurements describe basic properties of metrics that matter, what values we want those metrics to have, and how we’ll react if we can’t provide the expected service.
                

Location: 1,023
An SLI is a service level indicator — a carefully defined quantitative measure of some aspect of the level of service that is provided.
                

Location: 1,030
client-side latency is often the more user-relevant metric, but it might only be possible to measure latency at the server.
                

Location: 1,041
An SLO is a service level objective: a target value or range of values for a service level that is measured by an SLI. A natural structure for SLOs is thus SLI ≤ target, or lower bound ≤ SLI ≤ upper bound.
                

Location: 1,104
some systems should be instrumented with client-side collection, because not measuring behavior at the client can miss a range of problems that affect users but don’t affect server-side metrics.
                

Location: 1,139
We recommend that you standardize on common definitions for SLIs so that you don’t have to reason about them from first principles each time.
                

Location: 1,147
Start by thinking about (or finding out!) what your users care about, not what you can measure.
                

Location: 1,152
SLOs should specify how they’re measured and the conditions under which they’re valid.
                

Location: 1,160
It’s both unrealistic and undesirable to insist that SLOs will be met 100% of the time: doing so can reduce the rate of innovation and deployment, require expensive, overly conservative solutions, or both. Instead, it is better to allow an error budget — a rate at which the SLOs can be missed — and track that on a daily or weekly basis.
                

Location: 1,180
Choose just enough SLOs to provide good coverage of your system’s attributes.
                

Location: 1,184
You can always refine SLO definitions and targets over time as you learn about a system’s behavior.
                

Location: 1,226
If a human operator needs to touch your system during normal operations, you have a bug. The definition of normal changes as your systems grow. Carla Geisser, Google SRE
                

Location: 1,228
In SRE, we want to spend time on long-term engineering project work instead of operational work. Because the term operational work may be misinterpreted, we use a specific word: toil.
                

Location: 1,238
Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.
                

Location: 1,245
Toil is work you do over and over.
                

Location: 1,246
If a machine could accomplish the task just as well as a human, or the need for the task could be designed away, that task is toil.
                

Location: 1,249
Toil is interrupt-driven and reactive, rather than strategy-driven and proactive.
                

Location: 1,252
If your service remains in the same state after you have finished a task, the task was probably toil.
                

Location: 1,255
If the work involved in a task scales up linearly with service size, traffic volume, or user count, that task is probably toil.
                

Location: 1,259
Our SRE organization has an advertised goal of keeping operational work (i.e., toil) below 50% of each SRE’s time.
                

Location: 1,269
A typical SRE has one week of primary on-call and one week of secondary on-call in each cycle (for discussion of primary versus secondary on-call shifts, see Chapter 11
                

Location: 1,280
Engineering work is novel and intrinsically requires human judgment. It produces a permanent improvement in your service, and is guided by a strategy.
                

Location: 1,285
Software engineering Involves writing or modifying code, in addition to any associated design and documentation work.
                

Location: 1,288
Systems engineering Involves configuring production systems, modifying configurations, or documenting systems in a way that produces lasting improvements from a one-time effort.
                

Location: 1,292
Toil Work directly tied to running a service that is repetitive, manual, etc.
                

Location: 1,293
Overhead Administrative work not tied directly to running a service.
                

Location: 1,295
Every SRE needs to spend at least 50% of their time on engineering work, when averaged over a few quarters or a year.
                

Location: 1,304
Toil becomes toxic when experienced in large quantities.
                

Location: 1,305
too much toil is bad, consider the following: Career stagnation
                

Location: 1,308
Low morale
                

Location: 1,311
Creates confusion
                

Location: 1,314
Slows progress
                

Location: 1,316
Sets precedent
                

Location: 1,319
Promotes attrition
                

Location: 1,322
Causes breach of faith
                

Location: 1,411
keep noise low and signal high,
                

Location: 1,412
Rules that generate alerts for humans should be simple to understand and represent a clear failure.
                

Location: 1,415
what’s broken, and why?
                

Location: 1,440
The four golden signals of monitoring are latency, traffic, errors, and saturation. If you can only measure four metrics of your user-facing system, focus on these four.
                

Location: 1,442
Latency
                

Location: 1,449
Traffic
                

Location: 1,454
Errors
                

Location: 1,460
Saturation
                

Location: 1,480
The simplest way to differentiate between a slow average and a very slow “tail” of requests is to collect request counts bucketed by latencies (suitable for rendering a histogram), rather than actual latencies: how many requests did I serve that took between 0 ms and 10 ms, between 10 ms and 30 ms, between 30 ms and 100 ms, between 100 ms and 300 ms, and so on?
                

Location: 1,518
When creating rules for monitoring and alerting, asking the following questions can help you avoid false positives and pager burnout:3 Does this rule detect an otherwise undetected condition that is urgent, actionable, and actively or imminently user-visible?4 Will I ever be able to ignore this alert, knowing it’s benign? When and why will I be able to ignore this alert, and how can I avoid this scenario?
                

Location: 1,522
Does this alert definitely indicate that users are being negatively affected? Are there detectable cases in which users aren’t being negatively impacted, such as drained traffic or test deployments, that should be filtered out? Can I take action in response to this alert? Is that action urgent, or could it wait until morning? Could the action be safely automated? Will that action be a long-term fix, or just a short-term workaround? Are other people getting paged for this issue, therefore rendering at least one of the pages unnecessary?
                

Location: 1,539
Every page that happens today distracts a human from improving the system for tomorrow, so there is often a case for taking a short-term hit to availability or performance in order to improve the long-term outlook for the system.
                

Location: 1,554
Ultimately, temporarily backing off on our alerts allowed us to make faster progress toward a better service.
                

Location: 1,567
it’s easy to build layers of unmaintainable technical debt by patching over problems instead of making real fixes.
                

Location: 1,570
Pages with rote, algorithmic responses should be a red
                

Location: 1,578
We review statistics about page frequency (usually expressed as incidents per shift, where an incident might be composed of a few related pages) in quarterly reports with management, ensuring that decision makers are kept up to date on the pager load and overall health of their teams.
                

Orange highlightLocation: 1,599
Besides black art, there is only automation and mechanization. Federico García Lorca (1898–1936), Spanish poet and playwright
                

Orange highlightLocation: 1,601
automation is a force multiplier, not a panacea.
                

Location: 1,602
doing automation thoughtlessly can create as many problems as it solves.
                

Location: 1,606
The Value of Automation
                

Location: 1,609
Consistency
                

Location: 1,621
the value of consistency is in many ways the primary value of automation.
                

Location: 1,622
A Platform Automation doesn’t just provide consistency. Designed and done properly, automatic systems also provide a platform that can be extended, applied to more systems, or perhaps even spun out for profit.
                

Location: 1,631
Faster Repairs There’s an additional benefit for systems where automation is used to resolve common faults in a system (a frequent situation for SRE-created automation).
                

Location: 1,639
Faster Action In the infrastructural situations where SRE automation tends to be deployed, humans don’t usually react as fast as machines.
                

Location: 1,646
Time Saving Finally, time saving is an oft-quoted rationale for automation. Although people cite this rationale for automation more than the others, in many ways the benefit is often less immediately calculable.
                

Location: 1,653
“If we are engineering processes and solutions that are not automatable, we continue having to staff humans to maintain the system. If we have to staff humans to do the work, we are feeding the machines with the blood, sweat, and tears of human beings. Think The Matrix with less special effects and more pissed off System Administrators.”
                

Location: 1,677
The Use Cases for Automation
                

Location: 1,681
User account creation
                

Location: 1,681
Cluster turnup and turndown for services Software or hardware installation preparation and decommissioning Rollouts of new software versions Runtime configuration changes A special case of runtime config changes: changes to your dependencies
                

Location: 1,803
Python unit test framework
                

Location: 1,806
ProdTest for DNS Service, showing how one failed test aborts the subsequent chain of tests
                

Location: 1,835
Sorry, we’re unable to display this type of content.
        

Location: 1,856
The most functional tools are usually written by those who use them.
                

Location: 1,869
Later on, after the realization that turnup processes had to be owned by the teams that owned the services fully sank in, we saw this as a way to approach cluster turnup as a Service-Oriented Architecture (SOA) problem:
                

Location: 1,875
evolution of turnup automation followed a path: Operator-triggered manual action (no automation) Operator-written, system-specific automation Externally maintained generic automation Internally maintained, system-specific automation Autonomous systems that need no human intervention
                

Location: 1,894
Automation development began. Initially automation consisted of simple Python scripts for operations such as the following: Service management: keeping services running (e.g., restarts after segfaults) Tracking what services were supposed to run on which machines Log message parsing: SSHing into each machine and looking for regexps
                

Location: 1,897
Automation eventually mutated into a proper
                

Location: 1,897
database that tracked machine state, and also incorporated more sophisticated monitoring tools. With the union set of the automation available, we could now automatically manage much of the lifecycle of machines: noticing when machines were broken, removing the services, sending them to repair, and restoring the configuration when they came back from repair.
                

Location: 1,902
treating a collection of machines as a managed sea of resources.
                

Location: 1,909
To echo the words of Ben Treynor Sloss: by taking the approach that this was a software problem, the initial automation bought us enough time to turn cluster management into something autonomous, as opposed to automated. We achieved this goal by bringing ideas related to data distribution, APIs, hub-and-spoke architectures, and classic distributed system software development to bear upon the domain of infrastructure management.

Location: 1,934
“Automation: Enabling Failure at Scale”
                

Location: 1,949
One step during decommission involves overwriting the full content of the disk of all the machines in the rack, after which point an independent system verifies the successful erase. We call this process “Diskerase.”
                

Location: 1,978
Release engineering is a relatively new and fast-growing discipline of software engineering that can be concisely described as building and delivering software [McN14a]
                

Location: 1,990
We have tools that report on a host of metrics, such as how much time it takes for a code change to be deployed into production (in other words, release velocity) and statistics on what features are being used in build configuration files [Ada15]. Most of these tools were envisioned and developed by release engineers.
                

Location: 2,017
Build tools must allow us to ensure consistency and repeatability.
                

Location: 2,101
Sisyphus, which is a general-purpose rollout automation framework developed by SRE.
                

Location: 2,102
A rollout is a logical unit of work that is composed of one or more individual tasks. Sisyphus provides a set of Python classes that can be extended to support any
                

Location: 2,161
More Information For more information on release engineering, see the following presentations, each of which has video available online: How Embracing Continuous Release Reduced Change Complexity, USENIX Release Engineering Summit West 2014, [Dic14] Maintaining Consistency in a Massively Parallel Environment, USENIX Configuration Management Summit 2013, [McN13] The 10 Commandments of Release Engineering, 2nd International Workshop on Release Engineering 2014, [McN14b] Distributing Software in a Massively Parallel Environment, LISA 2014, [McN14c]
                

Location: 2,175
The price of reliability is the pursuit of the utmost simplicity. C.A.R. Hoare, Turing Award lecture
                

Location: 2,189
SREs work to create procedures, practices, and tools that render software more reliable. At the same time, SREs ensure that this work has as little impact on developer agility as possible. In fact, SRE’s experience has found that reliable processes tend to actually increase developer agility: rapid, reliable production rollouts make changes in production easier to see.
                

Location: 2,198
As Fred Brooks suggests in his “No Silver Bullet” essay [Bro95], it is very important to consider the difference between essential complexity and accidental complexity.
                

Location: 2,214
At the risk of sounding extreme, when you consider a web service that’s expected to be available 24/7, to some extent, every new line of code written is a liability. SRE promotes practices that make it more likely that all code has an essential purpose, such as scrutinizing code to make sure that it actually drives business goals, routinely removing dead code, and building bloat detection into all levels of testing.
                

Location: 2,221
every line of code changed or added to a project creates the potential for introducing new defects and bugs. A smaller project is easier to understand, easier to test, and frequently has fewer defects.
                

Location: 2,228
Writing clear, minimal APIs is an essential aspect of managing simplicity in a software system.
                

Location: 2,236
The ability to make changes to parts of the system in isolation is essential to creating a supportable system.
                

Location: 2,245
A well-designed distributed system consists of collaborators, each of which has a clear and well-scoped purpose.
                

Location: 2,249
Release Simplicity Simple releases are generally better than complicated releases. It is much easier to measure and understand the impact of a single change rather than a batch of changes released simultaneously. If we release 100 unrelated changes to a system at the same time and performance gets worse, understanding which changes impacted performance, and how they did so, will take considerable effort or additional instrumentation.
                

Location: 2,257
software simplicity is a prerequisite to reliability.
                

Location: 2,269
Put simply, SREs run services — a set of related systems, operated for users, who may be internal or external — and are ultimately responsible for the health of these services. Successfully operating a service entails a wide range of activities: developing monitoring systems, planning capacity, responding to incidents, ensuring the root causes of outages are addressed, and so on.
                

Location: 2,282
Sorry, we’re unable to display this type of content.
        

Location: 2,282
Reliability
                

Location: 2,293
Once you’re aware that there is a problem, how do you make it go away? That doesn’t necessarily mean fixing it once and for all — maybe you can stop the bleeding by reducing the system’s precision or turning off some features temporarily, allowing it to gracefully degrade, or maybe you can direct traffic to another instance of the service that’s working properly. The details of the solution you choose to implement are necessarily specific to your service and your organization. Responding effectively to incidents, however, is something applicable to all teams.
                

Location: 2,306
Building a blameless postmortem culture is the first step in understanding what went wrong (and what went right!), as described
                

Location: 2,357
Time-Series Data
                

Location: 2,360
Hierarchy of Production Needs
                

Location: 2,367
Google’s monitoring systems don’t just measure simple metrics, such as the average response time of an unladen European web server; we also need to understand the distribution of those response times across all web servers in that region. This knowledge enables us to identify the factors contributing to the latency tail.
                

Location: 2,372
a large system should be designed to aggregate signals and prune outliers.
                

Location: 2,373
We need monitoring systems that allow us to alert for high-level service objectives, but retain the granularity to inspect individual components as needed.
                

Location: 2,379
job scheduling infrastructure Borg
                

Location: 2,380
Borgmon
                

Location: 2,381
Time-Series Monitoring Outside of Google This chapter describes the architecture and programming interface of an internal monitoring tool that was foundational for the growth and reliability of Google for almost 10 years…but how does that help you, our dear reader? In recent years, monitoring has undergone a Cambrian Explosion: Riemann, Heka, Bosun, and Prometheus have emerged as open source tools that are very similar to Borgmon’s time-series–based alerting. In particular, Prometheus1 shares many similarities with Borgmon, especially when you compare the two rule languages. The principles of variable collection and rule evaluation remain the same across all these tools and provide an environment with which you can experiment, and hopefully launch into production, the ideas inspired by this chapter.
                

Location: 2,506
When collecting Borgmon-style data, it’s better to use counters, because they don’t lose meaning when events occur between sampling intervals.
                

Location: 2,611
white-box monitoring does not provide a full picture of the system being monitored; relying solely upon white-box monitoring means that you aren’t aware of what the users see.
                

Location: 2,614
Google solve this coverage issue with Prober, which runs a protocol check against a target and reports success or failure.
                

Location: 2,617
use Prober to export histograms of response times by operation type and payload size so that they can slice and dice the user-visible performance.
                

Location: 2,655
SRE work, as SREs work to scale all aspects of their work to the global scale.
                

Location: 2,658
the idea of treating time-series data as a data source for generating alerts is now accessible to everyone through those open source tools like Prometheus, Riemann, Heka, and Bosun, and probably others by the time you read this.
                

Location: 2,691
The SRE teams are quite different from purely operational teams in that they place heavy emphasis on the use of engineering to approach problems.
                

Location: 2,694
Google hires people with a diverse background in systems and software engineering into SRE teams.
                

Location: 2,699
guardians of production systems,
                

Location: 2,700
on-call engineers take care of their assigned operations by managing outages that affect the team and performing and/or vetting production changes.
                

Location: 2,708
As soon as a page is received and acknowledged, the on-call engineer is expected to triage the problem and work toward its resolution, possibly involving other team members and escalating as needed.
                

Location: 2,710
lower priority alerts or software releases, can also be handled and/or vetted by the on-call engineer during business hours.
                

Location: 2,725
we strive to invest at least 50% of SRE time into engineering: of the remainder, no more than 25% can be spent on-call, leaving up to another 25% on other types of operational, nonproject work.
                

Location: 2,742
We’ve found that on average, dealing with the tasks involved in an on-call incident — root-cause analysis, remediation, and follow-up activities like writing a postmortem and fixing bugs — takes 6 hours.
                

Location: 2,751
time-off-in-lieu or straight cash compensation,
                

Location: 2,753
This compensation structure ensures incentivization to be involved in on-call duties as required by the team, but also promotes a balanced on-call work distribution and limits potential drawbacks of excessive on-call work, such as burnout or inadequate time for project work.
                

Location: 2,765
Stress hormones like cortisol and corticotropin-releasing hormone (CRH) are known to cause behavioral consequences — including fear — that can impair cognitive functions and cause suboptimal decision making [Chr09].
                

Location: 2,771
While intuition and quick reactions can seem like desirable traits in the middle of incident management, they have downsides. Intuition can be wrong and is often less supportable by obvious data. Thus, following intuition can lead an engineer to waste time pursuing a line of reasoning that is incorrect from the start.
                

Location: 2,773
Quick reactions are deep-rooted in habit, and habitual responses are unconsidered, which means they can be disastrous. The ideal methodology in incident management strikes the perfect balance of taking steps at the desired pace when enough data is available to make a reasonable decision while simultaneously critically examining your assumptions.
                

Pink highlightLocation: 2,778
A blameless postmortem culture ([Loo10], [All12])
                

Location: 2,789
when an incident occurs, it’s important to evaluate what went wrong, recognize what went well, and take action to prevent the same errors from recurring in the future.
                

Location: 2,801
symptoms of operational overload should be measurable, so that the goals can be quantified (e.g., number of daily tickets < 5, paging events per shift < 2).
                

Location: 2,802
Misconfigured monitoring is a common cause of operational overload.
                

Location: 2,810
1:1 alert/incident ratio.
                

Location: 2,813
SRE teams may have the option to “give back the pager” — SRE can ask the developer team to be exclusively on-call for the system until it
                

Location: 2,827
Being out of touch with production for long periods of time can lead to confidence issues, both in terms of overconfidence and underconfidence, while knowledge gaps are discovered only when an incident occurs.
                

Location: 2,845
Be warned that being an expert is more than understanding how a system is supposed to work. Expertise is gained by investigating why a system doesn’t work.
                

Location: 2,866
Sorry, we’re unable to display this type of content.
        

Pink highlightLocation: 2,873
Common Pitfalls
                

Location: 2,876
Looking at symptoms that aren’t relevant or misunderstanding the meaning of system metrics. Wild goose chases often result. Misunderstanding how to change the system, its inputs, or its environment, so as to safely and effectively test hypotheses. Coming up with wildly improbable theories about what’s wrong, or latching on to causes of past problems, reasoning that since it happened once, it must be happening again. Hunting down spurious correlations that are actually coincidences or are correlated with shared causes.
                

Location: 2,916
Your response should be appropriate for the problem’s impact:
                

Location: 2,919
Your first response in a major outage may be to start troubleshooting and try to find a root cause as quickly as possible. Ignore that instinct! Instead, your course of action should be to make the system work as well as it can under the circumstances.
                

Pink highlightLocation: 2,922
Stopping the bleeding should be your first priority;
                

Location: 2,924
Novice pilots are taught that their first responsibility in an emergency is to fly the airplane [Gaw09]; troubleshooting is secondary to getting the plane and everyone on it safely onto the ground.
                

Orange highlightLocation: 2,937
Dapper
                

Location: 2,947
You might even want to design your logging infrastructure so that you can turn it on as needed, quickly and selectively.
                

Location: 2,977
Dividing and conquering is a very useful general-purpose solution technique. In a multilayer system where work happens throughout a stack of components, it’s often best to start systematically from one end of the stack and work toward the other end, examining each component in turn.
                

Location: 2,983
Ask “what,” “where,” and “why”
                

Location: 2,988
Unpacking the Causes of a Symptom
                

Location: 2,988
Symptom: A Spanner cluster has high latency and RPCs to its servers are timing out. Why? The Spanner server tasks are using all their CPU time and can’t make progress on all the requests the clients send. Where in the server is the CPU time being used? Profiling the server shows it’s sorting entries in logs checkpointed to disk. Where in the log-sorting code is it being used? When evaluating a regular expression against paths to log files. Solutions: Rewrite the regular expression to avoid backtracking. Look in the codebase for similar patterns. Consider using RE2, which does not backtrack and guarantees linear runtime growth with input size.11
                

Location: 2,997
Recent changes to a system can be a productive place to start identifying what’s going wrong.
                

Location: 2,998
Well-designed systems should have extensive production logging to track new version deployments and configuration changes at all layers of the stack, from the server binaries handling user traffic down to the packages installed on individual nodes in the cluster.
                

Location: 3,004
Figure 12-2. Error rates graphed against deployment start and end times
                

Location: 3,079
Once you’ve found the factors that caused the problem, it’s time to write up notes on what went wrong with the system, how you tracked down the problem, how you fixed the problem, and how to prevent it from happening again. In other words, you need to write a postmortem (although ideally, the system is alive at this point!).
                

Location: 3,149
Building observability
                

Location: 3,150
Designing systems with well-understood and observable interfaces between components.
                

Location: 3,151
Ensuring that information is available in a consistent way

Location: 3,200
one trait that’s vital to the long-term health of an organization, and that consequently sets that organization apart from others, is how the people involved respond to an emergency.
                

Location: 3,208
First of all, don’t panic!
                

Location: 3,218
We identify some weaknesses or hidden dependencies and document follow-up actions to rectify the flaws we uncover.
                

Location: 3,287
canarying,
                

Location: 3,313
US teams handed off to their European counterparts, and SRE hatched a plan to prioritize reinstallations using a streamlined but manual process.
                

Location: 3,364
Encourage Proactive Testing
                

Location: 3,500
Best Practices for Incident Management Prioritize. Stop the bleeding, restore service, and preserve the
                

Location: 3,500
evidence for root-causing. Prepare. Develop and document your incident management procedures in advance, in consultation with incident participants. Trust. Give full autonomy within the assigned role to all incident participants. Introspect. Pay attention to your emotional state while responding to an incident. If you start to feel panicky or overwhelmed, solicit more support. Consider alternatives. Periodically consider your options and re-evaluate whether it still makes sense to continue what you’re doing or whether you should be taking another tack in incident response. Practice. Use the process routinely so it becomes second nature. Change it around. Were you incident commander last time? Take on a different role this time. Encourage every team member to acquire familiarity with each role.
                

Location: 3,530
Postmortems are expected after any significant undesirable event. Writing a postmortem is not punishment — it is a learning opportunity for the entire company.
                

Pink highlightLocation: 3,537
Blameless postmortems are a tenet of SRE culture.
                

Location: 3,539
If a culture of finger pointing and shaming individuals or teams for doing the “wrong” thing prevails, people will not bring issues to light for fear of punishment.
                

Location: 3,541
These industries nurture an environment where every “mistake” is seen as an opportunity to strengthen the system.
                

Location: 3,553
Best Practice: Avoid Blame and Keep It Constructive Blameless postmortems can be challenging to write, because the postmortem format clearly identifies the actions that led to the incident. Removing blame from a postmortem gives people the confidence to escalate issues without fear. It is also important not to stigmatize frequent production of postmortems by a person or team. An atmosphere of blame risks creating a culture in which incidents and issues are swept under the rug, leading to greater risk for the organization [Boy13].
                

Location: 3,771
Relationships Between Testing and Mean Time to Repair
                

Location: 3,850
For each configuration file, a separate configuration test examines production to see how a
                

Location: 3,851
particular binary is actually configured and reports discrepancies against that file.
                

Location: 3,852
Configuration tests are built and tested for a specific version of the checked-in configuration file.
                

Location: 4,111
Interpreted languages such as Python are commonly used for configuration files because their interpreters can be embedded, and some simple sandboxing is available to protect against nonmalicious coding errors.
                

Location: 4,239
Releasing a tool to an internal audience with high familiarity with the problem space means that a development team can launch and iterate more quickly. Internal users are typically more understanding when it comes to minimal UI and other alpha product issues.
                

Location: 4,250
Beyond the design of automation tools and other efforts to reduce the workload for engineers in SRE, software development projects can further benefit the SRE organization by attracting and helping to retain engineers with a broad variety of skills. The desirability of team diversity is doubly true for SRE, where a variety of backgrounds and problem-solving approaches can help prevent blind spots. To this end, Google always strives to staff its SRE teams with a mix of engineers with traditional software development experience and engineers with systems engineering experience.
                

Location: 4,309
Intent-based Capacity Planning.
                

Location: 4,315
high-order priorities like SLOs, production dependencies, and service infrastructure requirements, as opposed to low-level scrounging for resources.
                

Location: 4,346
intent-driven capacity planning must take these constraints into account.
                

Location: 4,376
Figure 18-1 and the explanations that follow it outline Auxon’s major components. Figure 18-1. The major components
                

Location: 4,481
An idea that we’ve termed agnosticism — writing the software to be generalized to allow myriad data sources as input — was a key principle of Auxon’s design. Agnosticism meant that customers weren’t required to commit to any one tool in order to use the Auxon framework.
                

Location: 4,533
Dedicated, noninterrupted, project work time is essential to any software development effort.
                

Location: 4,554
Create and communicate a clear message
                

Location: 4,559
Reducing the number of ways to perform the same task allows the entire department to benefit from the skills any single team has developed, thus making knowledge and effort portable across teams.
                

Location: 4,562
Evaluate your organization’s capabilities
                

Location: 4,570
Launch and iterate
                

Location: 4,576
Don’t lower your standards
                

Location: 4,628
DNS load balancing.
                

Location: 4,655
We analyze traffic changes and continuously update our list of known DNS resolvers with the approximate size of the user base behind a given resolver, which allows us to track the potential impact of any given resolver.
                

Location: 4,657
We estimate the geographical distribution of the users behind each tracked resolver to increase the chance that we direct those users to the best location. Estimating geographic distribution is particularly tricky
                

Location: 4,665
The third implication of the DNS middleman is related to caching. Given that authoritative nameservers cannot flush resolvers’ caches, DNS records need a relatively low TTL. This effectively sets a lower bound on how quickly DNS changes can be propagated to users.
                

Location: 4,696
consistent hashing
                

Location: 4,712
Our current VIP load balancing solution [Eis16] uses packet encapsulation. A network load balancer puts the forwarded packet into another IP packet with Generic Routing Encapsulation (GRE) [Han94], and uses a backend’s address as the destination.
                

Location: 5,002
All that said, we’ve learned (the hard way!) about one very dangerous pitfall of the Least-Loaded Round Robin approach: if a task is seriously unhealthy, it might start serving 100% errors. Depending on the nature of those errors, they may have very low latency; it’s frequently significantly faster to just return an “I’m unhealthy!” error than to actually process a request. As a result, clients might start sending a very large amount of traffic to the unhealthy task, erroneously thinking that the task is available, as opposed to fast-failing them!
                

Location: 5,006
sinkholing traffic.
                

Location: 5,032
CPU distribution before and after enabling Weighted Round Robin
                

Location: 5,058
A better solution is to measure capacity directly in available resources.
                

Location: 5,063
In a majority of cases (although certainly not in all), we’ve found that simply using CPU consumption as the signal for provisioning works well, for the following reasons:
                

Location: 5,098
We implemented client-side throttling through a technique we call adaptive throttling. Specifically, each client task keeps the following information for the last two minutes of its history:
                

Location: 5,252
Mandate that batch client jobs use a separate set of batch proxy backend tasks that do nothing but forward requests to the underlying backends and hand their responses back to the clients in a controlled way. Therefore, instead of “batch client → backend,” you have “batch client → batch proxy → backend.” In this case, when the very large job starts, only the batch proxy job suffers, shielding the actual backends (and higher-priority clients).
                

Location: 5,275
A well-behaved backend, supported by robust load balancing policies, should accept only the requests that it can process and reject the rest gracefully.
                

Location: 5,286
A cascading failure is a failure that grows over time as a result of positive feedback.
                

Location: 5,325
Excessively long queue lengths If there is insufficient capacity to handle all the requests at steady state, the server will saturate its queues.
                

Location: 5,345
“GC death spiral.”
                

Location: 5,383
Preventing Server Overload
                

Location: 5,391
Instrument the server to reject requests when overloaded
                

Location: 5,427
Load Shedding and Graceful Degradation
                

Location: 5,428
Load shedding drops some proportion of load by dropping traffic as the server approaches overload conditions.
                

Location: 5,443
Graceful degradation
                

Location: 5,455
You can make sure that graceful degradation stays working by regularly running a small subset of servers near overload in order to exercise this code path.
                

Location: 5,502
Consider having a server-wide retry budget.
                

Location: 5,677
Load test components until they break.
                

Location: 5,681
Load testing also reveals where the breaking point is, knowledge that’s fundamental to the capacity planning process.
                

Location: 5,712
Immediate Steps to Address Cascading Failures
                

Location: 5,716
Increase Resources
                

Location: 5,719
Stop Health Check Failures/Deaths
                

Location: 5,727
Restart Servers
                

Location: 5,733
Drop Traffic
                

Location: 5,745
Enter Degraded Modes
                

Location: 5,748
Eliminate Batch Load
                

Location: 5,752
Eliminate Bad Traffic
                

Location: 5,780
watchdog is often implemented as a thread that wakes up periodically to see whether work has been done since the last time it checked.
                

Location: 5,800
distributed consensus to be effective in building reliable and highly available systems that require a consistent view of some system state.
                

Location: 5,824
BASE (Basically Available, Soft state, and Eventual consistency).
                

Location: 5,830
eventual consistency
                

Location: 5,836
consistency problems should be solved at the database level.”
                

Location: 5,838
Systems need to be able to reliably synchronize critical state across multiple processes. Distributed consensus algorithms provide this functionality.
                

Location: 5,852
STONITH (Shoot The Other Node in the Head)
                

Location: 5,869
gossip protocol to discover each other
                

Location: 5,875
distributed consensus algorithms
                

Location: 5,881
asynchronous distributed consensus
                

Location: 5,921
Consul,
                

Location: 5,926
Replicated State Machines A replicated state machine (RSM) is a system that executes the same set of operations, in the same order, on several processes.
                

Location: 5,933
, replicated state machines are a system implemented at a logical layer above the consensus algorithm.
                

Location: 6,353
If you remember nothing else from this chapter, keep in mind the sorts of problems that distributed consensus can be used to solve, and the types of problems that can arise when ad hoc methods such as heartbeats are used instead of distributed consensus.
                

Location: 6,382
Cron’s failure domain is essentially just one machine. If the machine is not running, neither the cron scheduler nor the jobs it launches can run.2
                

Location: 6,401
Cron job owners can (and should!) monitor their cron jobs;
                

Location: 6,461
We deploy multiple replicas of the cron service and use the Paxos distributed consensus algorithm (see Chapter 23) to ensure they have consistent state.
                

Location: 6,524
Most infrastructure that launches logical jobs in datacenters (Mesos, for example) provides naming for those datacenter jobs, making it possible to look up the state of jobs, stop the jobs, or perform other maintenance.
                

Location: 6,543
In order to prevent the infinite growth of the Paxos log, we can simply take a snapshot of the current state, which means that we can reconstruct the state without needing to replay all state change log entries leading to the current state.
                

Location: 6,553
We do not store logs on our distributed filesystem.
                

Location: 6,567
When people think of a “daily cron job,” they commonly configure this job to run at midnight. This setup works just fine if the cron job launches on the same machine, but what if your cron job can spawn a MapReduce with thousands of workers?
                

Location: 6,641
If this problem is detected by engineers or cluster monitoring infrastructure, the response can make matters worse. For example, the “sensible” or “default” response to a hanging chunk is to immediately kill the job and then allow the job to restart, because the blockage may well be the result of nondeterministic factors. However, because pipeline implementations by design usually don’t include checkpointing, work on all chunks is restarted from the beginning, thereby wasting the time, CPU cycles, and human effort invested in the previous cycle.
                

Location: 6,683
“thundering herd”
                

Location: 6,704
Google Workflow
                

Location: 6,713
Google developed a system in 2003 called “Workflow” that makes continuous processing available at scale. Workflow uses the leader-follower (workers) distributed systems design pattern [Sha00] and the system prevalence design pattern.4 This combination enables very large-scale transactional data pipelines, ensuring correctness with exactly-once semantics.
                

Location: 6,750
To avoid the situation in which an orphaned worker may continue working on a work unit, thus destroying the work of the current worker, each output file opened by a worker has a unique name. In this way, even orphaned workers can continue writing independently of the master until they attempt to commit. Upon attempting a commit, they will be unable to do so because another worker holds the lease for that work unit. Furthermore, orphaned workers cannot destroy the work produced by a valid worker, because the unique filename scheme ensures that every worker is writing to a distinct file. In this way, the double correctness guarantee holds: the output files are always unique, and the pipeline state is always correct by virtue of tasks with leases.
                

Location: 6,756
Workflow also versions all tasks.
                

Location: 6,898
No one really wants to make backups; what people really want are restores
                

Location: 6,944
Delivering a Recovery System, Rather Than a Backup System
                

Location: 6,954
A team practices and demonstrates their ability to meet those SLOs.
                

Location: 7,016
How Google SRE Faces the Challenges of Data Integrity
                

Location: 7,027
The first layer is soft deletion (or “lazy deletion” in the case of developer API offerings), which has proven to be an effective defense against inadvertent data deletion scenarios. The second line of defense is backups and their related recovery methods. The third and final layer is regular data validation, covered in “Third Layer: Early Detection”. Across all these layers, the presence of replication is occasionally useful for data recovery in specific scenarios (although data recovery plans should not rely upon replication).
                

Location: 7,054
Google has also found that the most devastating acute data deletion cases are caused by application developers unfamiliar with existing code but working on deletion-related code, especially batch processing pipelines (e.g., an offline MapReduce or Hadoop pipeline). It’s advantageous to design your interfaces to hinder developers unfamiliar with your code from circumventing soft deletion features with new code.
                

Location: 7,070
lazy deletion
                

Location: 7,103
To sum up our advice for guarding against the 24 combinations of data integrity failure modes: addressing a broad range of scenarios at reasonable cost demands a tiered backup strategy.
                

Location: 7,104
The first tier comprises many frequent and quickly restored backups stored closest to the live datastores, perhaps using the same or similar storage technologies as the data sources.
                

Location: 7,107
The second tier comprises fewer backups retained for single-digit or low double-digit days on random access distributed filesystems local to the site.
                

Location: 7,111
Subsequent tiers take advantage of nearline storage such as dedicated tape libraries and offsite storage of the backup media (e.g., tapes or disk drives).
                

Location: 7,150
“Bad” data doesn’t sit idly by, it propagates.
                

Location: 7,207
Mature Google services provide on-call engineers with comprehensive documentation and tools to troubleshoot. For example, on-call engineers for Gmail are provided with: A suite of playbook entries describing how to respond to a validation failure alert
                

Location: 7,209
A data validation dashboard
                

Location: 7,211
Data validation APIs that make validators easy to add and refactor
                

Location: 7,224
Continuously test the recovery process as part of your normal operations
                

Location: 7,224
Set up alerts that fire when a recovery process fails to provide a heartbeat indication of its success
                

Pink highlightLocation: 7,227
If you take away just one lesson from this chapter, remember that you only know that you can recover your recent state if you actually do so
                

Location: 7,229
automate these tests whenever possible and then run them continuously.
                

Location: 7,230
The aspects of your recovery plan you should confirm are myriad: Are your backups valid and complete, or are they empty? Do you have sufficient machine resources to run all of the setup, restore, and post-processing tasks that comprise your recovery? Does the recovery process complete in reasonable wall time? Are you able to monitor the state of your recovery process as it progresses? Are you free of critical dependencies on resources outside of your control, such as access to an offsite media storage vault that isn’t available 24/7?
                

Location: 7,255
our success was the fruit of planning, adherence to best practices, hard work, and cooperation, and we were glad to see our investment in each of these elements pay off as well as it did.
                

Location: 7,257
Defense in Depth and Emergency Preparedness.
                

Location: 7,299
Parallel bug identification and recovery efforts
                

Location: 7,388
Humans lack discipline to continually exercise system components, so automation is your
                

Location: 7,390
Defense in Depth Even the most bulletproof system is susceptible to bugs and operator error.
                

Location: 7,511
Launch Coordination Engineer is an SRE role, LCEs are incentivized to prioritize reliability over other concerns.
                

Location: 7,513
Launch Process
                

Location: 7,516
Lightweight
                

Location: 7,517
Robust
                

Location: 7,518
Thorough
                

Location: 7,519
Scalable
                

Location: 7,520
Adaptable
                

Location: 7,525
Simplicity
                

Location: 7,526
A high touch approach
                

Location: 7,528
Fast common paths
                

Location: 7,530
Experience has demonstrated that engineers are likely to sidestep processes that they consider too burdensome or as adding insufficient value — especially when a team is already in crunch mode, and the launch process is seen as just another item blocking their launch.
                

Location: 7,535
Checklists are used to reduce failure and ensure consistency and completeness across a variety of disciplines.
                

Location: 7,544
Maintaining a manageable burden on developers requires careful curation of the checklist.
                

Location: 7,545
adding new questions to Google’s launch checklist required approval from a vice president.
                

Location: 7,546
Every question’s importance must be substantiated, ideally by a previous launch disaster.
                

Location: 7,546
Every instruction must be concrete, practical, and reasonable for developers to accomplish.
                

Location: 7,550
Once or twice a year a team member reviews the entire checklist to identify obsolete items, and then works with service owners and subject matter experts to modernize sections of the checklist.
                

Location: 7,560
This type of standardization helped to radically simplify the launch checklist: for example, long sections of the checklist dealing with requirements for rate limiting could be replaced with a single line that stated, “Implement rate limiting using system X.”
                

Location: 7,580
A checklist is instrumental to launching new services and products with reproducible reliability.
                

Location: 7,608
Example checklist questions Is this launch tied to a press release, advertisement, blog post, or other form of promotion? How much traffic and rate of growth do you expect during and after the launch? Have you obtained all the compute resources needed to support your traffic?
                

Location: 7,612
examine each component and dependency and identify the impact of its failure.
                

Location: 7,616
Example checklist questions Do you have any single points of
                

Location: 7,616
failure in your design? How do you mitigate unavailability of your dependencies?
                

Location: 7,619
Implement load shedding to reject new requests early in overload situations.
                

Location: 7,632
automation is never perfect, and every service has processes that need to be executed by a human: creating a new release, moving the service to a different data center, restoring data from backups, and so on. For reliability reasons, we strive to minimize single points of failure, which include humans.
                

Location: 7,634
These remaining processes should be documented before launch to ensure that the information is translated from an engineer’s mind onto paper while it is still fresh, and that it is available in an emergency.
                

Location: 7,637
Are there any manual processes required to keep the service running?
                

Location: 7,639
Document all manual processes. Document the process for moving your service to a new datacenter. Automate the process for building and releasing a new version.
                

Location: 7,650
Sometimes a launch depends on factors beyond company control.
                

Location: 7,656
What third-party code, data, services, or events does the service or the launch depend upon? Do any partners depend on your service? If so, do they need to be notified of your launch? What happens if you or the vendor can’t meet a hard launch deadline?
                

Location: 7,665
Contingency measures are another part of rollout planning.
                

Location: 7,668
Set up a launch plan that identifies actions to take to launch the service. Identify who is responsible for each item. Identify risk in the individual launch steps and implement contingency measures.
                

Location: 7,679
Google has developed a number of patterns that allow us to launch products and features gradually and thereby minimize risk;
                

Location: 7,681
Almost all updates to Google’s services proceed gradually, according to a defined process, with appropriate verification steps interspersed.
                

Location: 7,684
a rollout are usually called “canaries”
                

Location: 7,695
Feature Flag Frameworks
                

Location: 7,700
Sometimes you simply want to check whether a small tweak to the user interface improves the experience of your users.
                

Location: 7,702
sometimes you want to find out whether a small sample of users like using an early prototype of a new, hard-to-implement feature. You don’t want to spend months of engineering effort to harden a new feature to serve millions of users, only to find that the feature is a flop.
                

Location: 7,718
Dealing with Abusive Client Behavior
                

Location: 7,720
A new client that syncs every 60 seconds, as opposed to every 600 seconds, causes 10 times the load on the service.
                

Location: 7,731
Randomness also needs to be injected into other periodic processes.
                

Location: 7,735
The ability to control the behavior of a client from the server side has proven an important tool in the past. For an app on a device, such control might mean instructing the client to check in periodically with the server and download a configuration file.
                

Location: 7,760
In services running on the Java Virtual Machine (JVM), a similar effect of grinding to a halt is sometimes called “GC (garbage collection) thrashing.” In this scenario, the virtual machine’s internal memory management runs in increasingly closer cycles, trying to free up memory until most of the CPU time is consumed by memory management.
                

Location: 7,775
Within two years, the product deployment requirements in the launch checklist grew long and complex.
                

Location: 7,784
Production Reviews.
                

Location: 7,788
While each question on the LCE Checklist is simple, much complexity is built in to what prompted the question and the implications of its answer.
                

Location: 7,807
Growing operational load When running a service after it launches, operational load, the amount of manual and repetitive engineering needed to keep a system functioning, tends to grow over time unless efforts are made to control such load.
                

Location: 7,809
Noisiness of automated notifications, complexity of deployment procedures, and the overhead of manual maintenance work tend to increase over time and consume increasing amounts of the service owner’s bandwidth, leaving the team less time for feature development.
                

Location: 7,818
churn reduction policy that prohibits infrastructure engineers from releasing backward-incompatible features until they also automate the migration of their clients to the new feature.
                

Location: 7,841
operational overload.
                

Location: 7,877
SRE education practices
                

Location: 7,892
Sorry, we’re unable to display this type of content.
        

Location: 7,937
Learning about your stack(s) and subsystem(s) requires a starting point.
                

Location: 7,940
1) How a query enters the system Networking and datacenter fundamentals, frontend load balancing, proxies, etc. 2) Frontend serving Application frontend(s), query logging, user experience SLO(s), etc. 3) Mid-tier services Caches, backend load balancing 4) Infrastructure Backends, infrastructure, and compute resources 5) Tying it all together Debugging techniques, escalation procedures, and emergency scenarios
                

Location: 7,950
The Results Mixing Server (“Mixer”)
                

Location: 7,966
Which backends of this server are considered “in the critical path,” and why?
                

Location: 7,966
What aspects of this server could be simplified or automated?
                

Location: 7,967
Where do you think the first bottleneck is in this architecture? If that bottleneck were to be saturated, what steps could you take to alleviate it?
                

Location: 8,016
Being too procedural in the face of an outage, thus forgetting your analytical skills, can be the difference between getting stuck and finding the root cause.
                

Location: 8,024
Reverse Engineering a Production Service
                

Location: 8,043
drifts in their prior understanding
                

Location: 8,046
Five Practices for Aspiring On-Callers
                

Location: 8,051
“Those who cannot remember the past are condemned to repeat it.”
                

Location: 8,055
When writing a postmortem, keep in mind that its most appreciative audience might be an engineer who hasn’t yet been hired.
                

Location: 8,057
should collect and curate valuable postmortems to serve as educational resources for future newbies.
                

Location: 8,064
“tales of fail”
                

Location: 8,078
regular disaster role playing.
                

Location: 8,117
Documentation as Apprenticeship
                

Location: 8,118
“on-call learning checklist,”
                

Location: 8,120
shadow on-caller.
                

Location: 8,142
on-call learning checklist),
                

Location: 8,152
Tip Should an outage occur for which writing a postmortem is beneficial, the on-caller should include the newbie as a coauthor. Do not dump the writeup solely on the student, because it could be mislearned that postmortems are somehow grunt work to be passed off on those most junior. It would be a mistake to create such an impression.
                

Location: 8,193
Any complex system is as imperfect as its creators.
                

Location: 8,194
Operational load
                

Location: 8,196
Pages
                

Location: 8,199
Tickets
                

Location: 8,202
Ongoing operational responsibilities
                

Location: 8,202
“Kicking the can down the road” and “toil”;
                

Location: 8,210
The primary on-call engineer might also manage user support communications, escalation to product developers, and so on. In order to both minimize the interruption a page causes to a team and avoid the bystander effect, Google on-call shifts are manned by a single engineer.
                

Location: 8,212
secondary on-call engineer acts as a backup for the primary.
                

Location: 8,229
Humans are imperfect machines.
                

Location: 8,234
The concept of flow state1 is widely accepted and can be empirically acknowledged by pretty much everyone who works in Software Engineering, Sysadmin, SRE, or most other disciplines that require focused periods of concentration.
                

Location: 8,255
interruptability
                

Location: 8,258
When you’re doing interrupts, your projects are a distraction
                

Location: 8,265
directing the structure of how the team itself manages interrupts, so that people aren’t set up for failure because of team function or structure.
                

Location: 8,283
In order to limit your distractibility, you should try to minimize context switches.
                

Location: 8,302
A person should never be expected to be on-call and also make progress on projects (or anything else with a high context switching cost)
                

Location: 8,304
If the function of the secondary is to back up the primary in the case of a fallthrough, then maybe you can safely assume that the secondary can also accomplish project work.
                

Location: 8,307
(Aside: You never run out of cleanup work. Your ticket count might be at zero, but there is always documentation that needs updating, configs that need cleanup, etc. Your future on-call engineers will thank you, and it means they’re less likely to interrupt you during your precious make time).
                
Location: 8,310
If you currently assign tickets randomly to victims on your team, stop. Doing so is extremely disrespectful of your team’s time, and works completely counter to the principle of not being interruptible as much as possible.
                

Location: 8,335
There should be a handoff for tickets, as well as for on-call work. A handoff process maintains shared state between ticket handlers as responsibility switches over.
                

Location: 8,338
Your team should conduct a regular scrub for tickets and pages, in which you examine classes of interrupts to see if you can identify a root cause.
                

Location: 8,339
silence the interrupts until the root cause is expected to be fixed
                

Location: 8,355
you need to find a balance between respect for the customer and for yourself.
                

Location: 8,367
When a team must allocate a disproportionate amount of time to resolving tickets at the cost of spending time improving the service, scalability and reliability suffer.
                

Location: 8,368
One way to relieve this burden is to temporarily transfer an SRE into the overloaded team.
                

Location: 8,381
Ops Mode Versus Nonlinear Scaling The term ops mode refers to a certain method of keeping a service running. Various work items increase with the size of the service. For example, a service needs a way to increase the number of configured virtual machines (VMs) as it grows. A team in ops mode responds by having a greater number of administrators managing those VMs. SRE instead focuses on writing software or eliminating scalability concerns so that the number of people required to run a service doesn’t increase as a function of load on the service.
                

Location: 8,392
SRE teams sometimes fall into ops mode because they focus on how to quickly address emergencies instead of how to reduce the number of emergencies.
                

Location: 8,397
Identify Kindling Once you identify a team’s largest existing problems, move on to emergencies waiting to happen.
                

Location: 8,440
Sort the team fires into toil and not-toil.
                

Location: 8,450
Your first goal for the team should be writing a service level objective (SLO), if one doesn’t already exist.
                

Location: 8,451
how important a process change could be. An SLO is probably the single most important lever for moving a team from reactive ops work to a healthy, long-term SRE focus.
                

Location: 8,457
Find useful work that can be accomplished by one team member. Clearly explain how this work addresses an issue from the postmortem in a permanent way. Even otherwise healthy teams can produce shortsighted action items. Serve as the reviewer for the code changes and document revisions. Repeat for two or three issues. When you identify an additional issue, put it in a bug report or a doc for the team to consult.
                

Location: 8,489
tenets outlined in this chapter provides an SRE team with the following: A technical, possibly quantitative, perspective on why they should change. A strong example of what change looks like. A logical explanation for much of the “folk wisdom” used by SRE. The core principles needed to address novel situations in a scalable manner.
                

Location: 8,492
Your final task is to write an after-action report.
                

Location: 8,532
production meeting
                

Location: 8,532
Production meetings are a special kind of meeting where an SRE team carefully articulates to itself — and to its invitees — the state of the service(s) in their charge, so as to increase general awareness among everyone who cares, and to improve the operation of the service(s).
                

Location: 8,535
The other major goal of production meetings is to improve our services by bringing the wisdom of production to bear on our services.
                

Location: 8,539
happen weekly;
                

Location: 8,543
chair. Many SRE teams rotate the chair through various team members,
                

Location: 8,554
Upcoming production changes
                

Location: 8,557
Metrics
                

Location: 8,559
core metrics of the systems in question;
                

Location: 8,563
Outages
                

Location: 8,566
Paging events
                

Location: 8,570
Nonpaging events
                

Location: 8,571
An issue that probably should have paged, but didn’t
                

Location: 8,573
An issue that is not pageable but requires attention, such as low-impact data corruption or slowness in some non-user-facing dimension of the system.
                

Location: 8,575
An issue that is not pageable and does not require attention.
                

Location: 8,576
Prior action items
                

Location: 8,582
Attendance is compulsory
                

Location: 8,598
Pre-populating the agenda
                

Location: 8,599
Fully use the multiple-person collaboration features enabled by the product.
                

Location: 8,621
Formally, SRE teams have the roles of “tech lead” (TL), “manager” (SRM), and “project manager” (also known as PM, TPM, PgM).
                

Location: 8,742
collaboration between the product development organization and SRE is really at its best when it occurs early on in the design phase, ideally before any line of code has been committed.
                

Location: 8,830
Production Readiness Review (PRR), a process that identifies the reliability needs of a service based on its specific details.
                

Location: 8,837
A typical service lifecycle
                

Location: 8,840
production. These aspects include the following: System architecture and interservice dependencies Instrumentation, metrics, and monitoring
                

Location: 8,842
Emergency response Capacity planning Change management Performance: availability, latency, and efficiency
                

Location: 8,870
The objectives of the Production Readiness Review are as follows: Verify that a service meets accepted standards of production setup and operational readiness, and that service owners are prepared to work with SRE and take advantage of SRE expertise. Improve the reliability of the service in production, and minimize the number and severity of incidents that might be expected. A PRR targets all aspects of production that SRE cares about.
                

Location: 8,882
Establishing an SLO/SLA for the service Planning for potentially disruptive design changes required to improve reliability Planning and training schedules
                

Location: 8,886
the SRE reviewers learn about the service and begin analyzing it for production shortcomings.
                

Location: 8,891
Do updates to the service impact an unreasonably large percentage of the system at once? Does the service connect to the
                

Location: 8,892
appropriate serving instance of its dependencies? For example, end-user requests to a service should not depend on a system that is designed for a batch-processing use case.
                

Location: 8,893
Does the service request a sufficiently high network quality-of-service when talking to a critical remote service? Does the service report errors to central logging systems for analysis? Does it report all exceptional conditions that result in degraded responses or failures to the end users? Are all user-visible request failures well instrumented and monitored, with suitable alerting configured?
                

Location: 8,900
Improvements and Refactoring
                

Location: 8,964
The Build phase addresses production aspects such as instrumentation and metrics, operational and emergency controls, resource usage, and efficiency.
                

Location: 8,969
“dark launch”
                

Location: 8,993
Onboarding each service required two or three SREs and typically lasted two or three quarters.
                

Location: 9,005
Google is increasingly following the industry trend of moving toward microservices.
                

Location: 9,015
Codified best practices
                

Location: 9,017
Reusable solutions
                

Location: 9,019
A common production platform with a common control surface
                

Location: 9,021
Easier automation and smarter systems A common control surface that
                

Location: 9,032
Framework modules address the various SRE concerns enumerated earlier, such as: Instrumentation and metrics Request logging Control systems involving traffic and load management
                

Location: 9,034
SRE builds framework modules to implement canonical solutions for the concerned production area.
                

Location: 9,037
framework might provide the following: Business logic organized as well-defined semantic components that can be referenced using standard terms Standard dimensions for monitoring instrumentation A standard format for request debugging logs A standard configuration format for managing load shedding Capacity of a single server and determination of “overload” that can both use a semantically consistent measure for feedback to various control systems
                

Location: 9,157
“Hope is not a strategy.”
                

Location: 9,158
The SRE culture is forever vigilant and constantly questioning: What could go wrong? What action can we take to address those issues before they lead to an outage or data loss?
                

Location: 9,174
Attention to Detail
                

Location: 9,175
how a lack of diligence in executing small tasks
                

Location: 9,176
A very small oversight or mistake can have big effects.
                

Location: 9,263
At their core, Google’s Site Reliability Engineers are software engineers with a low tolerance for repetitive reactive work. It is strongly ingrained in our culture to avoid repeating an operation that doesn’t add value to a service.
                

Location: 9,265
repetitive work that is of low value? Automation lowers operational overhead and frees up time for our engineers to proactively assess and improve the services they support.
                

Location: 9,304
Decisions should be informed rather than prescriptive, and are made without deference to personal opinions — even that of the most-senior person in the room, who Eric Schmidt and Jonathan Rosenberg dub the “HiPPO,” for “Highest-Paid Person’s Opinion”
                

Location: 9,311
Many industries heavily focus on playbooks and procedures rather than open-ended problem solving. Every humanly conceivable scenario is captured in a checklist or in “the binder.” When something goes wrong, this resource is the authoritative source for how to react. This prescriptive approach works for industries that evolve and develop relatively slowly, because the scenarios of what could go wrong are not constantly evolving due to system updates or changes. This approach is also common in industries in which the skill level of the workers may be limited, and the best way to make sure that people will respond appropriately in an emergency is to provide a simple, clear set of instructions.

Location: 9,376
ultimately, they still need to remain reliable, flexible, easy to manage in an emergency, well monitored, and capacity planned.
                

Location: 9,378
“automate discovery, dashboard building, and alerting over a fleet of tens of thousands of machines.”
                

Location: 9,399
An SRE team should be as compact as possible and operate at a high level of abstraction, relying upon lots of backup systems as failsafes and thoughtful APIs to communicate with the systems. At the same time, the SRE team should also have comprehensive knowledge of the systems — how they operate, how they fail, and how to respond to failures — that comes from operating them day-to-day.
                

Location: 9,409
Availability table
                

Location: 9,488
Postmortems Postmortems (see Chapter 15) should be blameless and focus on process and technology, not people. Assume the people involved in an incident are intelligent, are well intentioned, and were making the best choices they could given the information they had available at the time.
                

Location: 9,518
SRE teams should spend no more than 50% of their time on operational work (see Chapter 5); operational overflow should be directed to the product development team.

