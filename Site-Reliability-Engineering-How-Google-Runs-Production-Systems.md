Site Reliability Engineering: How Google Runs Production Systems
Betsy Beyer, Chris Jones, Jennifer Petoff, and Niall Richard Murphy

<img src="https://github.com/jeffjryan/images/blob/master/SRE.jpg" width=200>

Last accessed on Monday August 7, 2017
501 Highlight(s)
                
## Yellow highlight | Location: 56
Site Reliability Engineer.
                

## Yellow highlight | Location: 158
SRE Way in mind: thoroughness and dedication, belief in the value of preparation and documentation, and an awareness of what could go wrong, coupled with a strong desire to prevent it.
                

## Yellow highlight | Location: 288
Ben Treynor Sloss, the senior VP overseeing technical operations at Google — and the originator of the term “Site Reliability Engineering” —
                

## Yellow highlight | Location: 313
Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.
                

## Yellow highlight | Location: 321
At their core, the development teams want to launch new features and see them adopted by users. At their core, the ops teams want to make sure the service doesn’t break while they are holding the pager.
                

## Yellow highlight | Location: 323
Because most outages are caused by some kind of change — a new configuration, a new feature launch, or a new type of user traffic — the two teams’ goals are fundamentally in tension.
                

## Yellow highlight | Location: 335
Site Reliability Engineering teams focus on hiring software engineers to run our products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.
                

## Yellow highlight | Location: 338
SRE is what happens when you ask a software engineer to design an operations team.
                

## Yellow highlight | Location: 352
The result of our approach to hiring for SRE is that we end up with a team of people who (a) will quickly become bored by performing tasks by hand, and (b) have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.
                

## Yellow highlight | Location: 358
Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload.
                

Pink highlight | Location: 360
To avoid this fate, the team tasked with managing a service needs to code or it will drown. Therefore, Google places a 50% cap on the aggregate “ops” work for all SREs — tickets, on-call, manual tasks, etc. This cap ensures that the SRE team has enough time in their schedule to make the service stable and operable.
                

## Yellow highlight | Location: 364
we want systems that are automatic, not just automated.
                

## Yellow highlight | Location: 366
Google’s rule of thumb is that an SRE team must spend the remaining 50% of its time actually doing development.
                

## Yellow highlight | Location: 370
creative, autonomous engineering,
                

## Yellow highlight | Location: 376
the number of SREs needed to run, maintain, and improve a system scales sublinearly with the size of the system.
                

## Yellow highlight | Location: 393
Tenets of SRE
                

## Yellow highlight | Location: 396
In general, an SRE team is responsible for the availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning of their service(s).
                

## Yellow highlight | Location: 402
Google caps operational work for SREs at 50% of their time. Their remaining time should be spent using their coding skills on project work.
                

## Yellow highlight | Location: 403
redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, [re]integrating developers into on-call pager rotations, and so on. The redirection ends when the operational load drops back to 50% or lower. This also provides an effective feedback mechanism, guiding developers to build systems that don’t need manual intervention.
                

## Yellow highlight | Location: 409
When they are focused on operations work, on average, SREs should receive a maximum of two events per 8–12-hour on-call shift.
                

## Yellow highlight | Location: 414
Postmortems should be written for all significant incidents, regardless of whether or not they paged; postmortems that did not trigger a page are even more valuable, as they likely point to clear monitoring gaps.
                

## Yellow highlight | Location: 416
Google operates under a blame-free postmortem culture, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them.
                

## Yellow highlight | Location: 424
100% is the wrong reliability target for basically everything (pacemakers and anti-lock brakes being notable exceptions).
                

Pink highlight | Location: 427
the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.
                

## Yellow highlight | Location: 439
SRE’s goal is no longer “zero outages”; rather, SREs and product developers aim to spend the error budget getting maximum feature velocity.
                

Pink highlight | Location: 447
Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.
                

Blue highlight | Location: 449
There are three kinds of valid monitoring output:
                

Blue highlight | Location: 450
Alerts
                

Blue highlight | Location: 452
Tickets
                

Blue highlight | Location: 454
Logging
                

## Yellow highlight | Location: 455
The expectation is that no one reads logs unless something else prompts them to do so.
                

## Yellow highlight | Location: 459
Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR)
                

## Yellow highlight | Location: 463
When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a “playbook” produces roughly a 3x improvement in MTTR as compared to the strategy of “winging it.”
                

Pink highlight | Location: 467
on-call playbooks,
                

Pink highlight | Location: 472
SRE has found that roughly 70% of outages are due to changes in a live system.
                

## Yellow highlight | Location: 478
Demand forecasting and capacity planning can be viewed as ensuring that there is sufficient capacity and redundancy to serve projected future demand with the required availability.
                

Blue highlight | Location: 483
Several steps are mandatory in capacity planning: An accurate organic demand forecast, which extends beyond the lead time required for acquiring capacity An accurate incorporation of inorganic demand sources into the demand forecast Regular load testing of the system to correlate raw capacity (servers, disks, and so on) to service capacity
                

## Yellow highlight | Location: 497
Resource use is a function of demand (load), capacity, and software efficiency.
                

## Yellow highlight | Location: 500
Software systems become slower as load is added to them. A slowdown in a service equates to a loss of capacity.
                

Blue highlight | Location: 507
— it has become much more: a set of principles, a set of practices, a set of incentives, and a field of endeavor within the larger software engineering discipline.
                

## Yellow highlight | Location: 576
Because tasks are fluidly allocated over machines, we can’t simply rely on IP addresses and port numbers to refer to the tasks. We solve this problem with an extra level of indirection: when starting a job, Borg allocates a name and index number to each task using the Borg Naming Service (BNS).
                

## Yellow highlight | Location: 601
Bigtable supports eventually consistent, cross-datacenter replication.
                

## Yellow highlight | Location: 609
OpenFlow-based software-defined network.
                

## Yellow highlight | Location: 612
Bandwidth Enforcer
                

## Yellow highlight | Location: 617
Global Software Load Balancer (GSLB)
                

## Yellow highlight | Location: 627
asynchronous Consensus
                

## Yellow highlight | Location: 631
Monitoring and Alerting
                

## Yellow highlight | Location: 634
Borgmon monitoring program
                

## Yellow highlight | Location: 669
Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.
                

## Yellow highlight | Location: 768
Site Reliability Engineering seeks to balance the risk of unavailability with the goals of rapid innovation and efficient service operations,
                

## Yellow highlight | Location: 780
In SRE, we manage service reliability largely by managing risk. We conceptualize risk as a continuum. We give equal importance to figuring out how to engineer greater reliability into Google systems and identifying the appropriate level of tolerance for the services we run.
                

## Yellow highlight | Location: 783
We strive to make a service reliable enough, but no more reliable than it needs to be.
                

## Yellow highlight | Location: 794
unplanned downtime
                

## Yellow highlight | Location: 799
Time-based availability
                

## Yellow highlight | Location: 800
Sorry, we’re unable to display this type of content.
        

## Yellow highlight | Location: 808
instead of using metrics around uptime, we define availability in terms of the request success rate
                

Blue highlight | Location: 810
Sorry, we’re unable to display this type of content.
        

Blue highlight | Location: 813
In a typical application, not all requests are equal: failing a new user sign-up request is different from failing a request polling for new email in the background. In many cases, however, availability calculated as the request success rate over all requests is a reasonable approximation of unplanned downtime, as viewed from the end-user perspective.
                

## Yellow highlight | Location: 816
Most nonserving systems (e.g., batch, pipeline, storage, and transactional systems) have a well-defined notion of successful and unsuccessful units of work.
                

## Yellow highlight | Location: 820
request success rate
                

## Yellow highlight | Location: 821
we set quarterly availability targets for a service and track our performance against those targets on a weekly, or even daily, basis.
                

## Yellow highlight | Location: 838
There are many factors to consider when assessing the risk tolerance of services, such as the following: What level of availability is required? Do different types of failures have different effects on the service? How can we use the service cost to help locate a service on the risk continuum? What other service metrics are important to take into account? Target level of availability The target level of availability for a given Google service usually depends on the function it provides and how the service is positioned in the marketplace. The following list includes issues to consider: What level of service will the users expect? Does this service tie directly to revenue (either our revenue, or our customers’ revenue)? Is this a paid service, or is it free? If there are competitors in the marketplace, what level of service do those competitors provide? Is this service targeted at consumers, or at enterprises?
                

## Yellow highlight | Location: 926
The key strategy with regards to infrastructure is to deliver services with explicitly delineated levels of service, thus enabling the clients to make the right risk and cost trade-offs when building their systems.
                

## Yellow highlight | Location: 962
Canary duration and size
                

## Yellow highlight | Location: 963
It’s a best practice to test a new release on some small subset of a typical workload, a practice often called canarying. How long do we wait, and how big is the canary?
                

## Yellow highlight | Location: 969
Error Budget
                

## Yellow highlight | Location: 983
Many products use this control loop to manage release velocity: as long as the system’s SLOs are met, releases can continue. If SLO violations occur frequently enough to expend the error budget, releases are temporarily halted while additional resources are invested in system testing and development to make the system more resilient, improve its performance, and so
                

## Yellow highlight | Location: 997
Key Insights Managing service reliability is largely about managing risk, and managing risk can be costly. 100% is probably never the right reliability target: not only is it impossible to achieve, it’s typically more reliability than a service’s users want or notice. Match the profile of the service to the risk the business is willing to take. An error budget aligns incentives and emphasizes joint ownership between SRE and product development. Error budgets make it easier to decide the rate of releases and to effectively defuse discussions about outages with stakeholders, and allows multiple teams to reach the same conclusion about production risk without rancor.
                

## Yellow highlight | Location: 1,012
We use intuition, experience, and an understanding of what users want to define service level indicators (SLIs), objectives (SLOs), and agreements (SLAs). These measurements describe basic properties of metrics that matter, what values we want those metrics to have, and how we’ll react if we can’t provide the expected service.
                

## Yellow highlight | Location: 1,023
An SLI is a service level indicator — a carefully defined quantitative measure of some aspect of the level of service that is provided.
                

## Yellow highlight | Location: 1,030
client-side latency is often the more user-relevant metric, but it might only be possible to measure latency at the server.
                

## Yellow highlight | Location: 1,041
An SLO is a service level objective: a target value or range of values for a service level that is measured by an SLI. A natural structure for SLOs is thus SLI ≤ target, or lower bound ≤ SLI ≤ upper bound.
                

## Yellow highlight | Location: 1,104
some systems should be instrumented with client-side collection, because not measuring behavior at the client can miss a range of problems that affect users but don’t affect server-side metrics.
                

## Yellow highlight | Location: 1,139
We recommend that you standardize on common definitions for SLIs so that you don’t have to reason about them from first principles each time.
                

## Yellow highlight | Location: 1,147
Start by thinking about (or finding out!) what your users care about, not what you can measure.
                

## Yellow highlight | Location: 1,152
SLOs should specify how they’re measured and the conditions under which they’re valid.
                

## Yellow highlight | Location: 1,160
It’s both unrealistic and undesirable to insist that SLOs will be met 100% of the time: doing so can reduce the rate of innovation and deployment, require expensive, overly conservative solutions, or both. Instead, it is better to allow an error budget — a rate at which the SLOs can be missed — and track that on a daily or weekly basis.
                

## Yellow highlight | Location: 1,180
Choose just enough SLOs to provide good coverage of your system’s attributes.
                

## Yellow highlight | Location: 1,184
You can always refine SLO definitions and targets over time as you learn about a system’s behavior.
                

## Yellow highlight | Location: 1,226
If a human operator needs to touch your system during normal operations, you have a bug. The definition of normal changes as your systems grow. Carla Geisser, Google SRE
                

## Yellow highlight | Location: 1,228
In SRE, we want to spend time on long-term engineering project work instead of operational work. Because the term operational work may be misinterpreted, we use a specific word: toil.
                

## Yellow highlight | Location: 1,238
Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.
                

## Yellow highlight | Location: 1,245
Toil is work you do over and over.
                

## Yellow highlight | Location: 1,246
If a machine could accomplish the task just as well as a human, or the need for the task could be designed away, that task is toil.
                

## Yellow highlight | Location: 1,249
Toil is interrupt-driven and reactive, rather than strategy-driven and proactive.
                

## Yellow highlight | Location: 1,252
If your service remains in the same state after you have finished a task, the task was probably toil.
                

## Yellow highlight | Location: 1,255
If the work involved in a task scales up linearly with service size, traffic volume, or user count, that task is probably toil.
                

## Yellow highlight | Location: 1,259
Our SRE organization has an advertised goal of keeping operational work (i.e., toil) below 50% of each SRE’s time.
                

## Yellow highlight | Location: 1,269
A typical SRE has one week of primary on-call and one week of secondary on-call in each cycle (for discussion of primary versus secondary on-call shifts, see Chapter 11
                

## Yellow highlight | Location: 1,280
Engineering work is novel and intrinsically requires human judgment. It produces a permanent improvement in your service, and is guided by a strategy.
                

## Yellow highlight | Location: 1,285
Software engineering Involves writing or modifying code, in addition to any associated design and documentation work.
                

## Yellow highlight | Location: 1,288
Systems engineering Involves configuring production systems, modifying configurations, or documenting systems in a way that produces lasting improvements from a one-time effort.
                

## Yellow highlight | Location: 1,292
Toil Work directly tied to running a service that is repetitive, manual, etc.
                

## Yellow highlight | Location: 1,293
Overhead Administrative work not tied directly to running a service.
                

## Yellow highlight | Location: 1,295
Every SRE needs to spend at least 50% of their time on engineering work, when averaged over a few quarters or a year.
                

## Yellow highlight | Location: 1,304
Toil becomes toxic when experienced in large quantities.
                

## Yellow highlight | Location: 1,305
too much toil is bad, consider the following: Career stagnation
                

## Yellow highlight | Location: 1,308
Low morale
                

## Yellow highlight | Location: 1,311
Creates confusion
                

## Yellow highlight | Location: 1,314
Slows progress
                

## Yellow highlight | Location: 1,316
Sets precedent
                

## Yellow highlight | Location: 1,319
Promotes attrition
                

## Yellow highlight | Location: 1,322
Causes breach of faith
                

## Yellow highlight | Location: 1,411
keep noise low and signal high,
                

## Yellow highlight | Location: 1,412
Rules that generate alerts for humans should be simple to understand and represent a clear failure.
                

## Yellow highlight | Location: 1,415
what’s broken, and why?
                

## Yellow highlight | Location: 1,440
The four golden signals of monitoring are latency, traffic, errors, and saturation. If you can only measure four metrics of your user-facing system, focus on these four.
                

## Yellow highlight | Location: 1,442
Latency
                

## Yellow highlight | Location: 1,449
Traffic
                

## Yellow highlight | Location: 1,454
Errors
                

## Yellow highlight | Location: 1,460
Saturation
                

## Yellow highlight | Location: 1,480
The simplest way to differentiate between a slow average and a very slow “tail” of requests is to collect request counts bucketed by latencies (suitable for rendering a histogram), rather than actual latencies: how many requests did I serve that took between 0 ms and 10 ms, between 10 ms and 30 ms, between 30 ms and 100 ms, between 100 ms and 300 ms, and so on?
                

## Yellow highlight | Location: 1,518
When creating rules for monitoring and alerting, asking the following questions can help you avoid false positives and pager burnout:3 Does this rule detect an otherwise undetected condition that is urgent, actionable, and actively or imminently user-visible?4 Will I ever be able to ignore this alert, knowing it’s benign? When and why will I be able to ignore this alert, and how can I avoid this scenario?
                

## Yellow highlight | Location: 1,522
Does this alert definitely indicate that users are being negatively affected? Are there detectable cases in which users aren’t being negatively impacted, such as drained traffic or test deployments, that should be filtered out? Can I take action in response to this alert? Is that action urgent, or could it wait until morning? Could the action be safely automated? Will that action be a long-term fix, or just a short-term workaround? Are other people getting paged for this issue, therefore rendering at least one of the pages unnecessary?
                

## Yellow highlight | Location: 1,539
Every page that happens today distracts a human from improving the system for tomorrow, so there is often a case for taking a short-term hit to availability or performance in order to improve the long-term outlook for the system.
                

## Yellow highlight | Location: 1,554
Ultimately, temporarily backing off on our alerts allowed us to make faster progress toward a better service.
                

## Yellow highlight | Location: 1,567
it’s easy to build layers of unmaintainable technical debt by patching over problems instead of making real fixes.
                

## Yellow highlight | Location: 1,570
Pages with rote, algorithmic responses should be a red
                

## Yellow highlight | Location: 1,578
We review statistics about page frequency (usually expressed as incidents per shift, where an incident might be composed of a few related pages) in quarterly reports with management, ensuring that decision makers are kept up to date on the pager load and overall health of their teams.
                

Orange highlight | Location: 1,599
Besides black art, there is only automation and mechanization. Federico García Lorca (1898–1936), Spanish poet and playwright
                

Orange highlight | Location: 1,601
automation is a force multiplier, not a panacea.
                

## Yellow highlight | Location: 1,602
doing automation thoughtlessly can create as many problems as it solves.
                

## Yellow highlight | Location: 1,606
The Value of Automation
                

## Yellow highlight | Location: 1,609
Consistency
                

## Yellow highlight | Location: 1,621
the value of consistency is in many ways the primary value of automation.
                

## Yellow highlight | Location: 1,622
A Platform Automation doesn’t just provide consistency. Designed and done properly, automatic systems also provide a platform that can be extended, applied to more systems, or perhaps even spun out for profit.
                

## Yellow highlight | Location: 1,631
Faster Repairs There’s an additional benefit for systems where automation is used to resolve common faults in a system (a frequent situation for SRE-created automation).
                

## Yellow highlight | Location: 1,639
Faster Action In the infrastructural situations where SRE automation tends to be deployed, humans don’t usually react as fast as machines.
                

## Yellow highlight | Location: 1,646
Time Saving Finally, time saving is an oft-quoted rationale for automation. Although people cite this rationale for automation more than the others, in many ways the benefit is often less immediately calculable.
                

## Yellow highlight | Location: 1,653
“If we are engineering processes and solutions that are not automatable, we continue having to staff humans to maintain the system. If we have to staff humans to do the work, we are feeding the machines with the blood, sweat, and tears of human beings. Think The Matrix with less special effects and more pissed off System Administrators.”
                

Blue highlight | Location: 1,677
The Use Cases for Automation
                

## Yellow highlight | Location: 1,681
User account creation
                

## Yellow highlight | Location: 1,681
Cluster turnup and turndown for services Software or hardware installation preparation and decommissioning Rollouts of new software versions Runtime configuration changes A special case of runtime config changes: changes to your dependencies
                

Blue highlight | Location: 1,803
Python unit test framework
                

## Yellow highlight | Location: 1,806
ProdTest for DNS Service, showing how one failed test aborts the subsequent chain of tests
                

## Yellow highlight | Location: 1,835
Sorry, we’re unable to display this type of content.
        

## Yellow highlight | Location: 1,856
The most functional tools are usually written by those who use them.
                

## Yellow highlight | Location: 1,869
Later on, after the realization that turnup processes had to be owned by the teams that owned the services fully sank in, we saw this as a way to approach cluster turnup as a Service-Oriented Architecture (SOA) problem:
                

## Yellow highlight | Location: 1,875
evolution of turnup automation followed a path: Operator-triggered manual action (no automation) Operator-written, system-specific automation Externally maintained generic automation Internally maintained, system-specific automation Autonomous systems that need no human intervention
                

## Yellow highlight | Location: 1,894
Automation development began. Initially automation consisted of simple Python scripts for operations such as the following: Service management: keeping services running (e.g., restarts after segfaults) Tracking what services were supposed to run on which machines Log message parsing: SSHing into each machine and looking for regexps
                

## Yellow highlight | Location: 1,897
Automation eventually mutated into a proper
                

## Yellow highlight | Location: 1,897
database that tracked machine state, and also incorporated more sophisticated monitoring tools. With the union set of the automation available, we could now automatically manage much of the lifecycle of machines: noticing when machines were broken, removing the services, sending them to repair, and restoring the configuration when they came back from repair.
                

## Yellow highlight | Location: 1,902
treating a collection of machines as a managed sea of resources.
                

## Yellow highlight | Location: 1,909
To echo the words of Ben Treynor Sloss: by taking the approach that this was a software problem, the initial automation bought us enough time to turn cluster management into something autonomous, as opposed to automated. We achieved this goal by bringing ideas related to data distribution, APIs, hub-and-spoke architectures, and classic distributed system software development to bear upon the domain of infrastructure management.
                
Note:Key point

## Yellow highlight | Location: 1,934
“Automation: Enabling Failure at Scale”
                

## Yellow highlight | Location: 1,949
One step during decommission involves overwriting the full content of the disk of all the machines in the rack, after which point an independent system verifies the successful erase. We call this process “Diskerase.”
                

## Yellow highlight | Location: 1,978
Release engineering is a relatively new and fast-growing discipline of software engineering that can be concisely described as building and delivering software [McN14a]
                

## Yellow highlight | Location: 1,990
We have tools that report on a host of metrics, such as how much time it takes for a code change to be deployed into production (in other words, release velocity) and statistics on what features are being used in build configuration files [Ada15]. Most of these tools were envisioned and developed by release engineers.
                

## Yellow highlight | Location: 2,017
Build tools must allow us to ensure consistency and repeatability.
                

## Yellow highlight | Location: 2,101
Sisyphus, which is a general-purpose rollout automation framework developed by SRE.
                

## Yellow highlight | Location: 2,102
A rollout is a logical unit of work that is composed of one or more individual tasks. Sisyphus provides a set of Python classes that can be extended to support any
                

## Yellow highlight | Location: 2,161
More Information For more information on release engineering, see the following presentations, each of which has video available online: How Embracing Continuous Release Reduced Change Complexity, USENIX Release Engineering Summit West 2014, [Dic14] Maintaining Consistency in a Massively Parallel Environment, USENIX Configuration Management Summit 2013, [McN13] The 10 Commandments of Release Engineering, 2nd International Workshop on Release Engineering 2014, [McN14b] Distributing Software in a Massively Parallel Environment, LISA 2014, [McN14c]
                

## Yellow highlight | Location: 2,175
The price of reliability is the pursuit of the utmost simplicity. C.A.R. Hoare, Turing Award lecture
                

## Yellow highlight | Location: 2,189
SREs work to create procedures, practices, and tools that render software more reliable. At the same time, SREs ensure that this work has as little impact on developer agility as possible. In fact, SRE’s experience has found that reliable processes tend to actually increase developer agility: rapid, reliable production rollouts make changes in production easier to see.
                

## Yellow highlight | Location: 2,198
As Fred Brooks suggests in his “No Silver Bullet” essay [Bro95], it is very important to consider the difference between essential complexity and accidental complexity.
                

## Yellow highlight | Location: 2,214
At the risk of sounding extreme, when you consider a web service that’s expected to be available 24/7, to some extent, every new line of code written is a liability. SRE promotes practices that make it more likely that all code has an essential purpose, such as scrutinizing code to make sure that it actually drives business goals, routinely removing dead code, and building bloat detection into all levels of testing.
                

## Yellow highlight | Location: 2,221
every line of code changed or added to a project creates the potential for introducing new defects and bugs. A smaller project is easier to understand, easier to test, and frequently has fewer defects.
                

## Yellow highlight | Location: 2,228
Writing clear, minimal APIs is an essential aspect of managing simplicity in a software system.
                

## Yellow highlight | Location: 2,236
The ability to make changes to parts of the system in isolation is essential to creating a supportable system.
                

## Yellow highlight | Location: 2,245
A well-designed distributed system consists of collaborators, each of which has a clear and well-scoped purpose.
                

## Yellow highlight | Location: 2,249
Release Simplicity Simple releases are generally better than complicated releases. It is much easier to measure and understand the impact of a single change rather than a batch of changes released simultaneously. If we release 100 unrelated changes to a system at the same time and performance gets worse, understanding which changes impacted performance, and how they did so, will take considerable effort or additional instrumentation.
                

## Yellow highlight | Location: 2,257
software simplicity is a prerequisite to reliability.
                

## Yellow highlight | Location: 2,269
Put simply, SREs run services — a set of related systems, operated for users, who may be internal or external — and are ultimately responsible for the health of these services. Successfully operating a service entails a wide range of activities: developing monitoring systems, planning capacity, responding to incidents, ensuring the root causes of outages are addressed, and so on.
                

## Yellow highlight | Location: 2,282
Sorry, we’re unable to display this type of content.
        

## Yellow highlight | Location: 2,282
Reliability
                

## Yellow highlight | Location: 2,293
Once you’re aware that there is a problem, how do you make it go away? That doesn’t necessarily mean fixing it once and for all — maybe you can stop the bleeding by reducing the system’s precision or turning off some features temporarily, allowing it to gracefully degrade, or maybe you can direct traffic to another instance of the service that’s working properly. The details of the solution you choose to implement are necessarily specific to your service and your organization. Responding effectively to incidents, however, is something applicable to all teams.
                

## Yellow highlight | Location: 2,306
Building a blameless postmortem culture is the first step in understanding what went wrong (and what went right!), as described
                

## Yellow highlight | Location: 2,357
Time-Series Data
                

## Yellow highlight | Location: 2,360
Hierarchy of Production Needs
                

## Yellow highlight | Location: 2,367
Google’s monitoring systems don’t just measure simple metrics, such as the average response time of an unladen European web server; we also need to understand the distribution of those response times across all web servers in that region. This knowledge enables us to identify the factors contributing to the latency tail.
                

## Yellow highlight | Location: 2,372
a large system should be designed to aggregate signals and prune outliers.
                

## Yellow highlight | Location: 2,373
We need monitoring systems that allow us to alert for high-level service objectives, but retain the granularity to inspect individual components as needed.
                

## Yellow highlight | Location: 2,379
job scheduling infrastructure Borg
                

## Yellow highlight | Location: 2,380
Borgmon
                

## Yellow highlight | Location: 2,381
Time-Series Monitoring Outside of Google This chapter describes the architecture and programming interface of an internal monitoring tool that was foundational for the growth and reliability of Google for almost 10 years…but how does that help you, our dear reader? In recent years, monitoring has undergone a Cambrian Explosion: Riemann, Heka, Bosun, and Prometheus have emerged as open source tools that are very similar to Borgmon’s time-series–based alerting. In particular, Prometheus1 shares many similarities with Borgmon, especially when you compare the two rule languages. The principles of variable collection and rule evaluation remain the same across all these tools and provide an environment with which you can experiment, and hopefully launch into production, the ideas inspired by this chapter.
                

## Yellow highlight | Location: 2,506
When collecting Borgmon-style data, it’s better to use counters, because they don’t lose meaning when events occur between sampling intervals.
                

## Yellow highlight | Location: 2,611
white-box monitoring does not provide a full picture of the system being monitored; relying solely upon white-box monitoring means that you aren’t aware of what the users see.
                

## Yellow highlight | Location: 2,614
Google solve this coverage issue with Prober, which runs a protocol check against a target and reports success or failure.
                

## Yellow highlight | Location: 2,617
use Prober to export histograms of response times by operation type and payload size so that they can slice and dice the user-visible performance.
                

## Yellow highlight | Location: 2,655
SRE work, as SREs work to scale all aspects of their work to the global scale.
                

## Yellow highlight | Location: 2,658
the idea of treating time-series data as a data source for generating alerts is now accessible to everyone through those open source tools like Prometheus, Riemann, Heka, and Bosun, and probably others by the time you read this.
                

## Yellow highlight | Location: 2,691
The SRE teams are quite different from purely operational teams in that they place heavy emphasis on the use of engineering to approach problems.
                

## Yellow highlight | Location: 2,694
Google hires people with a diverse background in systems and software engineering into SRE teams.
                

Blue highlight | Location: 2,699
guardians of production systems,
                

## Yellow highlight | Location: 2,700
on-call engineers take care of their assigned operations by managing outages that affect the team and performing and/or vetting production changes.
                

## Yellow highlight | Location: 2,708
As soon as a page is received and acknowledged, the on-call engineer is expected to triage the problem and work toward its resolution, possibly involving other team members and escalating as needed.
                

## Yellow highlight | Location: 2,710
lower priority alerts or software releases, can also be handled and/or vetted by the on-call engineer during business hours.
                

## Yellow highlight | Location: 2,725
we strive to invest at least 50% of SRE time into engineering: of the remainder, no more than 25% can be spent on-call, leaving up to another 25% on other types of operational, nonproject work.
                

## Yellow highlight | Location: 2,742
We’ve found that on average, dealing with the tasks involved in an on-call incident — root-cause analysis, remediation, and follow-up activities like writing a postmortem and fixing bugs — takes 6 hours.
                

## Yellow highlight | Location: 2,751
time-off-in-lieu or straight cash compensation,
                

## Yellow highlight | Location: 2,753
This compensation structure ensures incentivization to be involved in on-call duties as required by the team, but also promotes a balanced on-call work distribution and limits potential drawbacks of excessive on-call work, such as burnout or inadequate time for project work.
                

## Yellow highlight | Location: 2,765
Stress hormones like cortisol and corticotropin-releasing hormone (CRH) are known to cause behavioral consequences — including fear — that can impair cognitive functions and cause suboptimal decision making [Chr09].
                

## Yellow highlight | Location: 2,771
While intuition and quick reactions can seem like desirable traits in the middle of incident management, they have downsides. Intuition can be wrong and is often less supportable by obvious data. Thus, following intuition can lead an engineer to waste time pursuing a line of reasoning that is incorrect from the start.
                

## Yellow highlight | Location: 2,773
Quick reactions are deep-rooted in habit, and habitual responses are unconsidered, which means they can be disastrous. The ideal methodology in incident management strikes the perfect balance of taking steps at the desired pace when enough data is available to make a reasonable decision while simultaneously critically examining your assumptions.
                

Pink highlight | Location: 2,778
A blameless postmortem culture ([Loo10], [All12])
                

Blue highlight | Location: 2,789
when an incident occurs, it’s important to evaluate what went wrong, recognize what went well, and take action to prevent the same errors from recurring in the future.
                

## Yellow highlight | Location: 2,801
symptoms of operational overload should be measurable, so that the goals can be quantified (e.g., number of daily tickets < 5, paging events per shift < 2).
                

## Yellow highlight | Location: 2,802
Misconfigured monitoring is a common cause of operational overload.
                

## Yellow highlight | Location: 2,810
1:1 alert/incident ratio.
                

## Yellow highlight | Location: 2,813
SRE teams may have the option to “give back the pager” — SRE can ask the developer team to be exclusively on-call for the system until it
                

## Yellow highlight | Location: 2,827
Being out of touch with production for long periods of time can lead to confidence issues, both in terms of overconfidence and underconfidence, while knowledge gaps are discovered only when an incident occurs.
                

## Yellow highlight | Location: 2,845
Be warned that being an expert is more than understanding how a system is supposed to work. Expertise is gained by investigating why a system doesn’t work.
                

## Yellow highlight | Location: 2,866
Sorry, we’re unable to display this type of content.
        

Pink highlight | Location: 2,873
Common Pitfalls
                

## Yellow highlight | Location: 2,876
Looking at symptoms that aren’t relevant or misunderstanding the meaning of system metrics. Wild goose chases often result. Misunderstanding how to change the system, its inputs, or its environment, so as to safely and effectively test hypotheses. Coming up with wildly improbable theories about what’s wrong, or latching on to causes of past problems, reasoning that since it happened once, it must be happening again. Hunting down spurious correlations that are actually coincidences or are correlated with shared causes.
                

## Yellow highlight | Location: 2,916
Your response should be appropriate for the problem’s impact:
                

Blue highlight | Location: 2,919
Your first response in a major outage may be to start troubleshooting and try to find a root cause as quickly as possible. Ignore that instinct! Instead, your course of action should be to make the system work as well as it can under the circumstances.
                

Pink highlight | Location: 2,922
Stopping the bleeding should be your first priority;
                

## Yellow highlight | Location: 2,924
Novice pilots are taught that their first responsibility in an emergency is to fly the airplane [Gaw09]; troubleshooting is secondary to getting the plane and everyone on it safely onto the ground.
                

Orange highlight | Location: 2,937
Dapper
                

## Yellow highlight | Location: 2,947
You might even want to design your logging infrastructure so that you can turn it on as needed, quickly and selectively.
                

## Yellow highlight | Location: 2,977
Dividing and conquering is a very useful general-purpose solution technique. In a multilayer system where work happens throughout a stack of components, it’s often best to start systematically from one end of the stack and work toward the other end, examining each component in turn.
                

Blue highlight | Location: 2,983
Ask “what,” “where,” and “why”
                

Blue highlight | Location: 2,988
Unpacking the Causes of a Symptom
                

## Yellow highlight | Location: 2,988
Symptom: A Spanner cluster has high latency and RPCs to its servers are timing out. Why? The Spanner server tasks are using all their CPU time and can’t make progress on all the requests the clients send. Where in the server is the CPU time being used? Profiling the server shows it’s sorting entries in logs checkpointed to disk. Where in the log-sorting code is it being used? When evaluating a regular expression against paths to log files. Solutions: Rewrite the regular expression to avoid backtracking. Look in the codebase for similar patterns. Consider using RE2, which does not backtrack and guarantees linear runtime growth with input size.11
                

## Yellow highlight | Location: 2,997
Recent changes to a system can be a productive place to start identifying what’s going wrong.
                

## Yellow highlight | Location: 2,998
Well-designed systems should have extensive production logging to track new version deployments and configuration changes at all layers of the stack, from the server binaries handling user traffic down to the packages installed on individual nodes in the cluster.
                

Blue highlight | Location: 3,004
Figure 12-2. Error rates graphed against deployment start and end times
                

Blue highlight | Location: 3,079
Once you’ve found the factors that caused the problem, it’s time to write up notes on what went wrong with the system, how you tracked down the problem, how you fixed the problem, and how to prevent it from happening again. In other words, you need to write a postmortem (although ideally, the system is alive at this point!).
                
Note:So we do detailed post motels, but where do we keep them, how searchable and accessible are they?  Something to look into.  Why not use confluence?

Blue highlight | Location: 3,149
Building observability
                

Blue highlight | Location: 3,150
Designing systems with well-understood and observable interfaces between components.
                

## Yellow highlight | Location: 3,151
Ensuring that information is available in a consistent way
                
Note:We need quick reactive Splunk queries that can be leveraged quickly for our variety of apps.

## Yellow highlight | Location: 3,200
one trait that’s vital to the long-term health of an organization, and that consequently sets that organization apart from others, is how the people involved respond to an emergency.
                

## Yellow highlight | Location: 3,208
First of all, don’t panic!
                

## Yellow highlight | Location: 3,218
We identify some weaknesses or hidden dependencies and document follow-up actions to rectify the flaws we uncover.
                

## Yellow highlight | Location: 3,287
canarying,
                

## Yellow highlight | Location: 3,313
US teams handed off to their European counterparts, and SRE hatched a plan to prioritize reinstallations using a streamlined but manual process.
                

## Yellow highlight | Location: 3,364
Encourage Proactive Testing
                

## Yellow highlight | Location: 3,500
Best Practices for Incident Management Prioritize. Stop the bleeding, restore service, and preserve the
                

## Yellow highlight | Location: 3,500
evidence for root-causing. Prepare. Develop and document your incident management procedures in advance, in consultation with incident participants. Trust. Give full autonomy within the assigned role to all incident participants. Introspect. Pay attention to your emotional state while responding to an incident. If you start to feel panicky or overwhelmed, solicit more support. Consider alternatives. Periodically consider your options and re-evaluate whether it still makes sense to continue what you’re doing or whether you should be taking another tack in incident response. Practice. Use the process routinely so it becomes second nature. Change it around. Were you incident commander last time? Take on a different role this time. Encourage every team member to acquire familiarity with each role.
                

## Yellow highlight | Location: 3,530
Postmortems are expected after any significant undesirable event. Writing a postmortem is not punishment — it is a learning opportunity for the entire company.
                

Pink highlight | Location: 3,537
Blameless postmortems are a tenet of SRE culture.
                

## Yellow highlight | Location: 3,539
If a culture of finger pointing and shaming individuals or teams for doing the “wrong” thing prevails, people will not bring issues to light for fear of punishment.
                

## Yellow highlight | Location: 3,541
These industries nurture an environment where every “mistake” is seen as an opportunity to strengthen the system.
                

## Yellow highlight | Location: 3,553
Best Practice: Avoid Blame and Keep It Constructive Blameless postmortems can be challenging to write, because the postmortem format clearly identifies the actions that led to the incident. Removing blame from a postmortem gives people the confidence to escalate issues without fear. It is also important not to stigmatize frequent production of postmortems by a person or team. An atmosphere of blame risks creating a culture in which incidents and issues are swept under the rug, leading to greater risk for the organization [Boy13].
                

## Yellow highlight | Location: 3,771
Relationships Between Testing and Mean Time to Repair
                

## Yellow highlight | Location: 3,850
For each configuration file, a separate configuration test examines production to see how a
                

## Yellow highlight | Location: 3,851
particular binary is actually configured and reports discrepancies against that file.
                

## Yellow highlight | Location: 3,852
Configuration tests are built and tested for a specific version of the checked-in configuration file.
                

## Yellow highlight | Location: 4,111
Interpreted languages such as Python are commonly used for configuration files because their interpreters can be embedded, and some simple sandboxing is available to protect against nonmalicious coding errors.
                

## Yellow highlight | Location: 4,239
Releasing a tool to an internal audience with high familiarity with the problem space means that a development team can launch and iterate more quickly. Internal users are typically more understanding when it comes to minimal UI and other alpha product issues.
                

## Yellow highlight | Location: 4,250
Beyond the design of automation tools and other efforts to reduce the workload for engineers in SRE, software development projects can further benefit the SRE organization by attracting and helping to retain engineers with a broad variety of skills. The desirability of team diversity is doubly true for SRE, where a variety of backgrounds and problem-solving approaches can help prevent blind spots. To this end, Google always strives to staff its SRE teams with a mix of engineers with traditional software development experience and engineers with systems engineering experience.
                

## Yellow highlight | Location: 4,309
Intent-based Capacity Planning.
                

## Yellow highlight | Location: 4,315
high-order priorities like SLOs, production dependencies, and service infrastructure requirements, as opposed to low-level scrounging for resources.
                

## Yellow highlight | Location: 4,346
intent-driven capacity planning must take these constraints into account.
                

## Yellow highlight | Location: 4,376
Figure 18-1 and the explanations that follow it outline Auxon’s major components. Figure 18-1. The major components
                

## Yellow highlight | Location: 4,481
An idea that we’ve termed agnosticism — writing the software to be generalized to allow myriad data sources as input — was a key principle of Auxon’s design. Agnosticism meant that customers weren’t required to commit to any one tool in order to use the Auxon framework.
                

## Yellow highlight | Location: 4,533
Dedicated, noninterrupted, project work time is essential to any software development effort.
                

## Yellow highlight | Location: 4,554
Create and communicate a clear message
                

## Yellow highlight | Location: 4,559
Reducing the number of ways to perform the same task allows the entire department to benefit from the skills any single team has developed, thus making knowledge and effort portable across teams.
                

## Yellow highlight | Location: 4,562
Evaluate your organization’s capabilities
                

## Yellow highlight | Location: 4,570
Launch and iterate
                

## Yellow highlight | Location: 4,576
Don’t lower your standards
                

## Yellow highlight | Location: 4,628
DNS load balancing.
                

## Yellow highlight | Location: 4,655
We analyze traffic changes and continuously update our list of known DNS resolvers with the approximate size of the user base behind a given resolver, which allows us to track the potential impact of any given resolver.
                

## Yellow highlight | Location: 4,657
We estimate the geographical distribution of the users behind each tracked resolver to increase the chance that we direct those users to the best location. Estimating geographic distribution is particularly tricky
                

## Yellow highlight | Location: 4,665
The third implication of the DNS middleman is related to caching. Given that authoritative nameservers cannot flush resolvers’ caches, DNS records need a relatively low TTL. This effectively sets a lower bound on how quickly DNS changes can be propagated to users.
                

## Yellow highlight | Location: 4,696
consistent hashing
                

## Yellow highlight | Location: 4,712
Our current VIP load balancing solution [Eis16] uses packet encapsulation. A network load balancer puts the forwarded packet into another IP packet with Generic Routing Encapsulation (GRE) [Han94], and uses a backend’s address as the destination.
                

## Yellow highlight | Location: 5,002
All that said, we’ve learned (the hard way!) about one very dangerous pitfall of the Least-Loaded Round Robin approach: if a task is seriously unhealthy, it might start serving 100% errors. Depending on the nature of those errors, they may have very low latency; it’s frequently significantly faster to just return an “I’m unhealthy!” error than to actually process a request. As a result, clients might start sending a very large amount of traffic to the unhealthy task, erroneously thinking that the task is available, as opposed to fast-failing them!
                

## Yellow highlight | Location: 5,006
sinkholing traffic.
                

## Yellow highlight | Location: 5,032
CPU distribution before and after enabling Weighted Round Robin
                

## Yellow highlight | Location: 5,058
A better solution is to measure capacity directly in available resources.
                

## Yellow highlight | Location: 5,063
In a majority of cases (although certainly not in all), we’ve found that simply using CPU consumption as the signal for provisioning works well, for the following reasons:
                

## Yellow highlight | Location: 5,098
We implemented client-side throttling through a technique we call adaptive throttling. Specifically, each client task keeps the following information for the last two minutes of its history:
                

## Yellow highlight | Location: 5,252
Mandate that batch client jobs use a separate set of batch proxy backend tasks that do nothing but forward requests to the underlying backends and hand their responses back to the clients in a controlled way. Therefore, instead of “batch client → backend,” you have “batch client → batch proxy → backend.” In this case, when the very large job starts, only the batch proxy job suffers, shielding the actual backends (and higher-priority clients).
                

## Yellow highlight | Location: 5,275
A well-behaved backend, supported by robust load balancing policies, should accept only the requests that it can process and reject the rest gracefully.
                

## Yellow highlight | Location: 5,286
A cascading failure is a failure that grows over time as a result of positive feedback.
                

## Yellow highlight | Location: 5,325
Excessively long queue lengths If there is insufficient capacity to handle all the requests at steady state, the server will saturate its queues.
                

## Yellow highlight | Location: 5,345
“GC death spiral.”
                

## Yellow highlight | Location: 5,383
Preventing Server Overload
                

## Yellow highlight | Location: 5,391
Instrument the server to reject requests when overloaded
                

## Yellow highlight | Location: 5,427
Load Shedding and Graceful Degradation
                

## Yellow highlight | Location: 5,428
Load shedding drops some proportion of load by dropping traffic as the server approaches overload conditions.
                

## Yellow highlight | Location: 5,443
Graceful degradation
                

## Yellow highlight | Location: 5,455
You can make sure that graceful degradation stays working by regularly running a small subset of servers near overload in order to exercise this code path.
                

## Yellow highlight | Location: 5,502
Consider having a server-wide retry budget.
                

## Yellow highlight | Location: 5,677
Load test components until they break.
                

## Yellow highlight | Location: 5,681
Load testing also reveals where the breaking point is, knowledge that’s fundamental to the capacity planning process.
                

## Yellow highlight | Location: 5,712
Immediate Steps to Address Cascading Failures
                

## Yellow highlight | Location: 5,716
Increase Resources
                

## Yellow highlight | Location: 5,719
Stop Health Check Failures/Deaths
                

## Yellow highlight | Location: 5,727
Restart Servers
                

## Yellow highlight | Location: 5,733
Drop Traffic
                

## Yellow highlight | Location: 5,745
Enter Degraded Modes
                

## Yellow highlight | Location: 5,748
Eliminate Batch Load
                

## Yellow highlight | Location: 5,752
Eliminate Bad Traffic
                

## Yellow highlight | Location: 5,780
watchdog is often implemented as a thread that wakes up periodically to see whether work has been done since the last time it checked.
                

## Yellow highlight | Location: 5,800
distributed consensus to be effective in building reliable and highly available systems that require a consistent view of some system state.
                

## Yellow highlight | Location: 5,824
BASE (Basically Available, Soft state, and Eventual consistency).
                

## Yellow highlight | Location: 5,830
eventual consistency
                

## Yellow highlight | Location: 5,836
consistency problems should be solved at the database level.”
                

## Yellow highlight | Location: 5,838
Systems need to be able to reliably synchronize critical state across multiple processes. Distributed consensus algorithms provide this functionality.
                

## Yellow highlight | Location: 5,852
STONITH (Shoot The Other Node in the Head)
                

## Yellow highlight | Location: 5,869
gossip protocol to discover each other
                

## Yellow highlight | Location: 5,875
distributed consensus algorithms
                

## Yellow highlight | Location: 5,881
asynchronous distributed consensus
                

## Yellow highlight | Location: 5,921
Consul,
                

## Yellow highlight | Location: 5,926
Replicated State Machines A replicated state machine (RSM) is a system that executes the same set of operations, in the same order, on several processes.
                

## Yellow highlight | Location: 5,933
, replicated state machines are a system implemented at a logical layer above the consensus algorithm.
                

## Yellow highlight | Location: 6,353
If you remember nothing else from this chapter, keep in mind the sorts of problems that distributed consensus can be used to solve, and the types of problems that can arise when ad hoc methods such as heartbeats are used instead of distributed consensus.
                

## Yellow highlight | Location: 6,382
Cron’s failure domain is essentially just one machine. If the machine is not running, neither the cron scheduler nor the jobs it launches can run.2
                

## Yellow highlight | Location: 6,401
Cron job owners can (and should!) monitor their cron jobs;
                

## Yellow highlight | Location: 6,461
We deploy multiple replicas of the cron service and use the Paxos distributed consensus algorithm (see Chapter 23) to ensure they have consistent state.
                

## Yellow highlight | Location: 6,524
Most infrastructure that launches logical jobs in datacenters (Mesos, for example) provides naming for those datacenter jobs, making it possible to look up the state of jobs, stop the jobs, or perform other maintenance.
                

## Yellow highlight | Location: 6,543
In order to prevent the infinite growth of the Paxos log, we can simply take a snapshot of the current state, which means that we can reconstruct the state without needing to replay all state change log entries leading to the current state.
                

## Yellow highlight | Location: 6,553
We do not store logs on our distributed filesystem.
                

## Yellow highlight | Location: 6,567
When people think of a “daily cron job,” they commonly configure this job to run at midnight. This setup works just fine if the cron job launches on the same machine, but what if your cron job can spawn a MapReduce with thousands of workers?
                

## Yellow highlight | Location: 6,641
If this problem is detected by engineers or cluster monitoring infrastructure, the response can make matters worse. For example, the “sensible” or “default” response to a hanging chunk is to immediately kill the job and then allow the job to restart, because the blockage may well be the result of nondeterministic factors. However, because pipeline implementations by design usually don’t include checkpointing, work on all chunks is restarted from the beginning, thereby wasting the time, CPU cycles, and human effort invested in the previous cycle.
                

## Yellow highlight | Location: 6,683
“thundering herd”
                

## Yellow highlight | Location: 6,704
Google Workflow
                

## Yellow highlight | Location: 6,713
Google developed a system in 2003 called “Workflow” that makes continuous processing available at scale. Workflow uses the leader-follower (workers) distributed systems design pattern [Sha00] and the system prevalence design pattern.4 This combination enables very large-scale transactional data pipelines, ensuring correctness with exactly-once semantics.
                

## Yellow highlight | Location: 6,750
To avoid the situation in which an orphaned worker may continue working on a work unit, thus destroying the work of the current worker, each output file opened by a worker has a unique name. In this way, even orphaned workers can continue writing independently of the master until they attempt to commit. Upon attempting a commit, they will be unable to do so because another worker holds the lease for that work unit. Furthermore, orphaned workers cannot destroy the work produced by a valid worker, because the unique filename scheme ensures that every worker is writing to a distinct file. In this way, the double correctness guarantee holds: the output files are always unique, and the pipeline state is always correct by virtue of tasks with leases.
                

## Yellow highlight | Location: 6,756
Workflow also versions all tasks.
                

## Yellow highlight | Location: 6,898
No one really wants to make backups; what people really want are restores
                

## Yellow highlight | Location: 6,944
Delivering a Recovery System, Rather Than a Backup System
                

## Yellow highlight | Location: 6,954
A team practices and demonstrates their ability to meet those SLOs.
                

## Yellow highlight | Location: 7,016
How Google SRE Faces the Challenges of Data Integrity
                

## Yellow highlight | Location: 7,027
The first layer is soft deletion (or “lazy deletion” in the case of developer API offerings), which has proven to be an effective defense against inadvertent data deletion scenarios. The second line of defense is backups and their related recovery methods. The third and final layer is regular data validation, covered in “Third Layer: Early Detection”. Across all these layers, the presence of replication is occasionally useful for data recovery in specific scenarios (although data recovery plans should not rely upon replication).
                

## Yellow highlight | Location: 7,054
Google has also found that the most devastating acute data deletion cases are caused by application developers unfamiliar with existing code but working on deletion-related code, especially batch processing pipelines (e.g., an offline MapReduce or Hadoop pipeline). It’s advantageous to design your interfaces to hinder developers unfamiliar with your code from circumventing soft deletion features with new code.
                

## Yellow highlight | Location: 7,070
lazy deletion
                

## Yellow highlight | Location: 7,103
To sum up our advice for guarding against the 24 combinations of data integrity failure modes: addressing a broad range of scenarios at reasonable cost demands a tiered backup strategy.
                

## Yellow highlight | Location: 7,104
The first tier comprises many frequent and quickly restored backups stored closest to the live datastores, perhaps using the same or similar storage technologies as the data sources.
                

## Yellow highlight | Location: 7,107
The second tier comprises fewer backups retained for single-digit or low double-digit days on random access distributed filesystems local to the site.
                

## Yellow highlight | Location: 7,111
Subsequent tiers take advantage of nearline storage such as dedicated tape libraries and offsite storage of the backup media (e.g., tapes or disk drives).
                

## Yellow highlight | Location: 7,150
“Bad” data doesn’t sit idly by, it propagates.
                

## Yellow highlight | Location: 7,207
Mature Google services provide on-call engineers with comprehensive documentation and tools to troubleshoot. For example, on-call engineers for Gmail are provided with: A suite of playbook entries describing how to respond to a validation failure alert
                

## Yellow highlight | Location: 7,209
A data validation dashboard
                

## Yellow highlight | Location: 7,211
Data validation APIs that make validators easy to add and refactor
                

## Yellow highlight | Location: 7,224
Continuously test the recovery process as part of your normal operations
                

## Yellow highlight | Location: 7,224
Set up alerts that fire when a recovery process fails to provide a heartbeat indication of its success
                

Pink highlight | Location: 7,227
If you take away just one lesson from this chapter, remember that you only know that you can recover your recent state if you actually do so
                

## Yellow highlight | Location: 7,229
automate these tests whenever possible and then run them continuously.
                

## Yellow highlight | Location: 7,230
The aspects of your recovery plan you should confirm are myriad: Are your backups valid and complete, or are they empty? Do you have sufficient machine resources to run all of the setup, restore, and post-processing tasks that comprise your recovery? Does the recovery process complete in reasonable wall time? Are you able to monitor the state of your recovery process as it progresses? Are you free of critical dependencies on resources outside of your control, such as access to an offsite media storage vault that isn’t available 24/7?
                

## Yellow highlight | Location: 7,255
our success was the fruit of planning, adherence to best practices, hard work, and cooperation, and we were glad to see our investment in each of these elements pay off as well as it did.
                

## Yellow highlight | Location: 7,257
Defense in Depth and Emergency Preparedness.
                

## Yellow highlight | Location: 7,299
Parallel bug identification and recovery efforts
                

## Yellow highlight | Location: 7,388
Humans lack discipline to continually exercise system components, so automation is your
                

## Yellow highlight | Location: 7,390
Defense in Depth Even the most bulletproof system is susceptible to bugs and operator error.
                

## Yellow highlight | Location: 7,511
Launch Coordination Engineer is an SRE role, LCEs are incentivized to prioritize reliability over other concerns.
                

## Yellow highlight | Location: 7,513
Launch Process
                

## Yellow highlight | Location: 7,516
Lightweight
                

## Yellow highlight | Location: 7,517
Robust
                

## Yellow highlight | Location: 7,518
Thorough
                

## Yellow highlight | Location: 7,519
Scalable
                

## Yellow highlight | Location: 7,520
Adaptable
                

## Yellow highlight | Location: 7,525
Simplicity
                

## Yellow highlight | Location: 7,526
A high touch approach
                

## Yellow highlight | Location: 7,528
Fast common paths
                

## Yellow highlight | Location: 7,530
Experience has demonstrated that engineers are likely to sidestep processes that they consider too burdensome or as adding insufficient value — especially when a team is already in crunch mode, and the launch process is seen as just another item blocking their launch.
                

## Yellow highlight | Location: 7,535
Checklists are used to reduce failure and ensure consistency and completeness across a variety of disciplines.
                

## Yellow highlight | Location: 7,544
Maintaining a manageable burden on developers requires careful curation of the checklist.
                

## Yellow highlight | Location: 7,545
adding new questions to Google’s launch checklist required approval from a vice president.
                

Note | Location: 7,545
Very important point.

## Yellow highlight | Location: 7,546
Every question’s importance must be substantiated, ideally by a previous launch disaster.
                

## Yellow highlight | Location: 7,546
Every instruction must be concrete, practical, and reasonable for developers to accomplish.
                

## Yellow highlight | Location: 7,550
Once or twice a year a team member reviews the entire checklist to identify obsolete items, and then works with service owners and subject matter experts to modernize sections of the checklist.
                

## Yellow highlight | Location: 7,560
This type of standardization helped to radically simplify the launch checklist: for example, long sections of the checklist dealing with requirements for rate limiting could be replaced with a single line that stated, “Implement rate limiting using system X.”
                

## Yellow highlight | Location: 7,580
A checklist is instrumental to launching new services and products with reproducible reliability.
                

## Yellow highlight | Location: 7,608
Example checklist questions Is this launch tied to a press release, advertisement, blog post, or other form of promotion? How much traffic and rate of growth do you expect during and after the launch? Have you obtained all the compute resources needed to support your traffic?
                

## Yellow highlight | Location: 7,612
examine each component and dependency and identify the impact of its failure.
                

## Yellow highlight | Location: 7,616
Example checklist questions Do you have any single points of
                

## Yellow highlight | Location: 7,616
failure in your design? How do you mitigate unavailability of your dependencies?
                

## Yellow highlight | Location: 7,619
Implement load shedding to reject new requests early in overload situations.
                

## Yellow highlight | Location: 7,632
automation is never perfect, and every service has processes that need to be executed by a human: creating a new release, moving the service to a different data center, restoring data from backups, and so on. For reliability reasons, we strive to minimize single points of failure, which include humans.
                

## Yellow highlight | Location: 7,634
These remaining processes should be documented before launch to ensure that the information is translated from an engineer’s mind onto paper while it is still fresh, and that it is available in an emergency.
                

## Yellow highlight | Location: 7,637
Are there any manual processes required to keep the service running?
                

## Yellow highlight | Location: 7,639
Document all manual processes. Document the process for moving your service to a new datacenter. Automate the process for building and releasing a new version.
                

## Yellow highlight | Location: 7,650
Sometimes a launch depends on factors beyond company control.
                

## Yellow highlight | Location: 7,656
What third-party code, data, services, or events does the service or the launch depend upon? Do any partners depend on your service? If so, do they need to be notified of your launch? What happens if you or the vendor can’t meet a hard launch deadline?
                

## Yellow highlight | Location: 7,665
Contingency measures are another part of rollout planning.
                

## Yellow highlight | Location: 7,668
Set up a launch plan that identifies actions to take to launch the service. Identify who is responsible for each item. Identify risk in the individual launch steps and implement contingency measures.
                

## Yellow highlight | Location: 7,679
Google has developed a number of patterns that allow us to launch products and features gradually and thereby minimize risk;
                

## Yellow highlight | Location: 7,681
Almost all updates to Google’s services proceed gradually, according to a defined process, with appropriate verification steps interspersed.
                

## Yellow highlight | Location: 7,684
a rollout are usually called “canaries”
                

## Yellow highlight | Location: 7,695
Feature Flag Frameworks
                

## Yellow highlight | Location: 7,700
Sometimes you simply want to check whether a small tweak to the user interface improves the experience of your users.
                

## Yellow highlight | Location: 7,702
sometimes you want to find out whether a small sample of users like using an early prototype of a new, hard-to-implement feature. You don’t want to spend months of engineering effort to harden a new feature to serve millions of users, only to find that the feature is a flop.
                

## Yellow highlight | Location: 7,718
Dealing with Abusive Client Behavior
                

## Yellow highlight | Location: 7,720
A new client that syncs every 60 seconds, as opposed to every 600 seconds, causes 10 times the load on the service.
                

## Yellow highlight | Location: 7,731
Randomness also needs to be injected into other periodic processes.
                

## Yellow highlight | Location: 7,735
The ability to control the behavior of a client from the server side has proven an important tool in the past. For an app on a device, such control might mean instructing the client to check in periodically with the server and download a configuration file.
                

## Yellow highlight | Location: 7,760
In services running on the Java Virtual Machine (JVM), a similar effect of grinding to a halt is sometimes called “GC (garbage collection) thrashing.” In this scenario, the virtual machine’s internal memory management runs in increasingly closer cycles, trying to free up memory until most of the CPU time is consumed by memory management.
                

## Yellow highlight | Location: 7,775
Within two years, the product deployment requirements in the launch checklist grew long and complex.
                

## Yellow highlight | Location: 7,784
Production Reviews.
                

## Yellow highlight | Location: 7,788
While each question on the LCE Checklist is simple, much complexity is built in to what prompted the question and the implications of its answer.
                

## Yellow highlight | Location: 7,807
Growing operational load When running a service after it launches, operational load, the amount of manual and repetitive engineering needed to keep a system functioning, tends to grow over time unless efforts are made to control such load.
                

## Yellow highlight | Location: 7,809
Noisiness of automated notifications, complexity of deployment procedures, and the overhead of manual maintenance work tend to increase over time and consume increasing amounts of the service owner’s bandwidth, leaving the team less time for feature development.
                

## Yellow highlight | Location: 7,818
churn reduction policy that prohibits infrastructure engineers from releasing backward-incompatible features until they also automate the migration of their clients to the new feature.
                

## Yellow highlight | Location: 7,841
operational overload.
                

## Yellow highlight | Location: 7,877
SRE education practices
                

## Yellow highlight | Location: 7,892
Sorry, we’re unable to display this type of content.
        

## Yellow highlight | Location: 7,937
Learning about your stack(s) and subsystem(s) requires a starting point.
                

## Yellow highlight | Location: 7,940
1) How a query enters the system Networking and datacenter fundamentals, frontend load balancing, proxies, etc. 2) Frontend serving Application frontend(s), query logging, user experience SLO(s), etc. 3) Mid-tier services Caches, backend load balancing 4) Infrastructure Backends, infrastructure, and compute resources 5) Tying it all together Debugging techniques, escalation procedures, and emergency scenarios
                

## Yellow highlight | Location: 7,950
The Results Mixing Server (“Mixer”)
                

## Yellow highlight | Location: 7,966
Which backends of this server are considered “in the critical path,” and why?
                
Note:Need diagrams of the system.

## Yellow highlight | Location: 7,966
What aspects of this server could be simplified or automated?
                

## Yellow highlight | Location: 7,967
Where do you think the first bottleneck is in this architecture? If that bottleneck were to be saturated, what steps could you take to alleviate it?
                

## Yellow highlight | Location: 8,016
Being too procedural in the face of an outage, thus forgetting your analytical skills, can be the difference between getting stuck and finding the root cause.
                

## Yellow highlight | Location: 8,024
Reverse Engineering a Production Service
                

## Yellow highlight | Location: 8,043
drifts in their prior understanding
                

## Yellow highlight | Location: 8,046
Five Practices for Aspiring On-Callers
                

## Yellow highlight | Location: 8,051
“Those who cannot remember the past are condemned to repeat it.”
                

## Yellow highlight | Location: 8,055
When writing a postmortem, keep in mind that its most appreciative audience might be an engineer who hasn’t yet been hired.
                

## Yellow highlight | Location: 8,057
should collect and curate valuable postmortems to serve as educational resources for future newbies.
                

## Yellow highlight | Location: 8,064
“tales of fail”
                

## Yellow highlight | Location: 8,078
regular disaster role playing.
                

## Yellow highlight | Location: 8,117
Documentation as Apprenticeship
                

## Yellow highlight | Location: 8,118
“on-call learning checklist,”
                

## Yellow highlight | Location: 8,120
shadow on-caller.
                

## Yellow highlight | Location: 8,142
on-call learning checklist),
                

## Yellow highlight | Location: 8,152
Tip Should an outage occur for which writing a postmortem is beneficial, the on-caller should include the newbie as a coauthor. Do not dump the writeup solely on the student, because it could be mislearned that postmortems are somehow grunt work to be passed off on those most junior. It would be a mistake to create such an impression.
                

## Yellow highlight | Location: 8,193
Any complex system is as imperfect as its creators.
                

## Yellow highlight | Location: 8,194
Operational load
                

## Yellow highlight | Location: 8,196
Pages
                

## Yellow highlight | Location: 8,199
Tickets
                

## Yellow highlight | Location: 8,202
Ongoing operational responsibilities
                

## Yellow highlight | Location: 8,202
“Kicking the can down the road” and “toil”;
                

## Yellow highlight | Location: 8,210
The primary on-call engineer might also manage user support communications, escalation to product developers, and so on. In order to both minimize the interruption a page causes to a team and avoid the bystander effect, Google on-call shifts are manned by a single engineer.
                

## Yellow highlight | Location: 8,212
secondary on-call engineer acts as a backup for the primary.
                

## Yellow highlight | Location: 8,229
Humans are imperfect machines.
                

## Yellow highlight | Location: 8,234
The concept of flow state1 is widely accepted and can be empirically acknowledged by pretty much everyone who works in Software Engineering, Sysadmin, SRE, or most other disciplines that require focused periods of concentration.
                

## Yellow highlight | Location: 8,255
interruptability
                

## Yellow highlight | Location: 8,258
When you’re doing interrupts, your projects are a distraction
                

## Yellow highlight | Location: 8,265
directing the structure of how the team itself manages interrupts, so that people aren’t set up for failure because of team function or structure.
                

## Yellow highlight | Location: 8,283
In order to limit your distractibility, you should try to minimize context switches.
                

## Yellow highlight | Location: 8,302
A person should never be expected to be on-call and also make progress on projects (or anything else with a high context switching cost)
                

## Yellow highlight | Location: 8,304
If the function of the secondary is to back up the primary in the case of a fallthrough, then maybe you can safely assume that the secondary can also accomplish project work.
                

## Yellow highlight | Location: 8,307
(Aside: You never run out of cleanup work. Your ticket count might be at zero, but there is always documentation that needs updating, configs that need cleanup, etc. Your future on-call engineers will thank you, and it means they’re less likely to interrupt you during your precious make time).
                

Note | Location: 8,309
Take heed, need to determine if we do this.

## Yellow highlight | Location: 8,310
If you currently assign tickets randomly to victims on your team, stop. Doing so is extremely disrespectful of your team’s time, and works completely counter to the principle of not being interruptible as much as possible.
                

## Yellow highlight | Location: 8,335
There should be a handoff for tickets, as well as for on-call work. A handoff process maintains shared state between ticket handlers as responsibility switches over.
                

## Yellow highlight | Location: 8,338
Your team should conduct a regular scrub for tickets and pages, in which you examine classes of interrupts to see if you can identify a root cause.
                

## Yellow highlight | Location: 8,339
silence the interrupts until the root cause is expected to be fixed
                

## Yellow highlight | Location: 8,355
you need to find a balance between respect for the customer and for yourself.
                

## Yellow highlight | Location: 8,367
When a team must allocate a disproportionate amount of time to resolving tickets at the cost of spending time improving the service, scalability and reliability suffer.
                

## Yellow highlight | Location: 8,368
One way to relieve this burden is to temporarily transfer an SRE into the overloaded team.
                

## Yellow highlight | Location: 8,381
Ops Mode Versus Nonlinear Scaling The term ops mode refers to a certain method of keeping a service running. Various work items increase with the size of the service. For example, a service needs a way to increase the number of configured virtual machines (VMs) as it grows. A team in ops mode responds by having a greater number of administrators managing those VMs. SRE instead focuses on writing software or eliminating scalability concerns so that the number of people required to run a service doesn’t increase as a function of load on the service.
                

## Yellow highlight | Location: 8,392
SRE teams sometimes fall into ops mode because they focus on how to quickly address emergencies instead of how to reduce the number of emergencies.
                

## Yellow highlight | Location: 8,397
Identify Kindling Once you identify a team’s largest existing problems, move on to emergencies waiting to happen.
                

## Yellow highlight | Location: 8,440
Sort the team fires into toil and not-toil.
                

## Yellow highlight | Location: 8,450
Your first goal for the team should be writing a service level objective (SLO), if one doesn’t already exist.
                

## Yellow highlight | Location: 8,451
how important a process change could be. An SLO is probably the single most important lever for moving a team from reactive ops work to a healthy, long-term SRE focus.
                

## Yellow highlight | Location: 8,457
Find useful work that can be accomplished by one team member. Clearly explain how this work addresses an issue from the postmortem in a permanent way. Even otherwise healthy teams can produce shortsighted action items. Serve as the reviewer for the code changes and document revisions. Repeat for two or three issues. When you identify an additional issue, put it in a bug report or a doc for the team to consult.
                

## Yellow highlight | Location: 8,489
tenets outlined in this chapter provides an SRE team with the following: A technical, possibly quantitative, perspective on why they should change. A strong example of what change looks like. A logical explanation for much of the “folk wisdom” used by SRE. The core principles needed to address novel situations in a scalable manner.
                

## Yellow highlight | Location: 8,492
Your final task is to write an after-action report.
                

## Yellow highlight | Location: 8,532
production meeting
                

## Yellow highlight | Location: 8,532
Production meetings are a special kind of meeting where an SRE team carefully articulates to itself — and to its invitees — the state of the service(s) in their charge, so as to increase general awareness among everyone who cares, and to improve the operation of the service(s).
                

## Yellow highlight | Location: 8,535
The other major goal of production meetings is to improve our services by bringing the wisdom of production to bear on our services.
                

## Yellow highlight | Location: 8,539
happen weekly;
                

## Yellow highlight | Location: 8,543
chair. Many SRE teams rotate the chair through various team members,
                

## Yellow highlight | Location: 8,554
Upcoming production changes
                

## Yellow highlight | Location: 8,557
Metrics
                

## Yellow highlight | Location: 8,559
core metrics of the systems in question;
                

## Yellow highlight | Location: 8,563
Outages
                

## Yellow highlight | Location: 8,566
Paging events
                

## Yellow highlight | Location: 8,570
Nonpaging events
                

## Yellow highlight | Location: 8,571
An issue that probably should have paged, but didn’t
                

## Yellow highlight | Location: 8,573
An issue that is not pageable but requires attention, such as low-impact data corruption or slowness in some non-user-facing dimension of the system.
                

## Yellow highlight | Location: 8,575
An issue that is not pageable and does not require attention.
                

## Yellow highlight | Location: 8,576
Prior action items
                

## Yellow highlight | Location: 8,582
Attendance is compulsory
                

## Yellow highlight | Location: 8,598
Pre-populating the agenda
                

## Yellow highlight | Location: 8,599
Fully use the multiple-person collaboration features enabled by the product.
                

## Yellow highlight | Location: 8,621
Formally, SRE teams have the roles of “tech lead” (TL), “manager” (SRM), and “project manager” (also known as PM, TPM, PgM).
                

## Yellow highlight | Location: 8,742
collaboration between the product development organization and SRE is really at its best when it occurs early on in the design phase, ideally before any line of code has been committed.
                

## Yellow highlight | Location: 8,830
Production Readiness Review (PRR), a process that identifies the reliability needs of a service based on its specific details.
                

## Yellow highlight | Location: 8,837
A typical service lifecycle
                

## Yellow highlight | Location: 8,840
production. These aspects include the following: System architecture and interservice dependencies Instrumentation, metrics, and monitoring
                

## Yellow highlight | Location: 8,842
Emergency response Capacity planning Change management Performance: availability, latency, and efficiency
                

## Yellow highlight | Location: 8,870
The objectives of the Production Readiness Review are as follows: Verify that a service meets accepted standards of production setup and operational readiness, and that service owners are prepared to work with SRE and take advantage of SRE expertise. Improve the reliability of the service in production, and minimize the number and severity of incidents that might be expected. A PRR targets all aspects of production that SRE cares about.
                

## Yellow highlight | Location: 8,882
Establishing an SLO/SLA for the service Planning for potentially disruptive design changes required to improve reliability Planning and training schedules
                

## Yellow highlight | Location: 8,886
the SRE reviewers learn about the service and begin analyzing it for production shortcomings.
                

## Yellow highlight | Location: 8,891
Do updates to the service impact an unreasonably large percentage of the system at once? Does the service connect to the
                

## Yellow highlight | Location: 8,892
appropriate serving instance of its dependencies? For example, end-user requests to a service should not depend on a system that is designed for a batch-processing use case.
                

## Yellow highlight | Location: 8,893
Does the service request a sufficiently high network quality-of-service when talking to a critical remote service? Does the service report errors to central logging systems for analysis? Does it report all exceptional conditions that result in degraded responses or failures to the end users? Are all user-visible request failures well instrumented and monitored, with suitable alerting configured?
                

## Yellow highlight | Location: 8,900
Improvements and Refactoring
                

## Yellow highlight | Location: 8,964
The Build phase addresses production aspects such as instrumentation and metrics, operational and emergency controls, resource usage, and efficiency.
                

## Yellow highlight | Location: 8,969
“dark launch”
                

## Yellow highlight | Location: 8,993
Onboarding each service required two or three SREs and typically lasted two or three quarters.
                

## Yellow highlight | Location: 9,005
Google is increasingly following the industry trend of moving toward microservices.
                

## Yellow highlight | Location: 9,015
Codified best practices
                

## Yellow highlight | Location: 9,017
Reusable solutions
                

## Yellow highlight | Location: 9,019
A common production platform with a common control surface
                

## Yellow highlight | Location: 9,021
Easier automation and smarter systems A common control surface that
                

## Yellow highlight | Location: 9,032
Framework modules address the various SRE concerns enumerated earlier, such as: Instrumentation and metrics Request logging Control systems involving traffic and load management
                

## Yellow highlight | Location: 9,034
SRE builds framework modules to implement canonical solutions for the concerned production area.
                

## Yellow highlight | Location: 9,037
framework might provide the following: Business logic organized as well-defined semantic components that can be referenced using standard terms Standard dimensions for monitoring instrumentation A standard format for request debugging logs A standard configuration format for managing load shedding Capacity of a single server and determination of “overload” that can both use a semantically consistent measure for feedback to various control systems
                

## Yellow highlight | Location: 9,157
“Hope is not a strategy.”
                

## Yellow highlight | Location: 9,158
The SRE culture is forever vigilant and constantly questioning: What could go wrong? What action can we take to address those issues before they lead to an outage or data loss?
                

## Yellow highlight | Location: 9,174
Attention to Detail
                

## Yellow highlight | Location: 9,175
how a lack of diligence in executing small tasks
                

## Yellow highlight | Location: 9,176
A very small oversight or mistake can have big effects.
                

## Yellow highlight | Location: 9,263
At their core, Google’s Site Reliability Engineers are software engineers with a low tolerance for repetitive reactive work. It is strongly ingrained in our culture to avoid repeating an operation that doesn’t add value to a service.
                

## Yellow highlight | Location: 9,265
repetitive work that is of low value? Automation lowers operational overhead and frees up time for our engineers to proactively assess and improve the services they support.
                

## Yellow highlight | Location: 9,304
Decisions should be informed rather than prescriptive, and are made without deference to personal opinions — even that of the most-senior person in the room, who Eric Schmidt and Jonathan Rosenberg dub the “HiPPO,” for “Highest-Paid Person’s Opinion”
                

## Yellow highlight | Location: 9,311
Many industries heavily focus on playbooks and procedures rather than open-ended problem solving. Every humanly conceivable scenario is captured in a checklist or in “the binder.” When something goes wrong, this resource is the authoritative source for how to react. This prescriptive approach works for industries that evolve and develop relatively slowly, because the scenarios of what could go wrong are not constantly evolving due to system updates or changes. This approach is also common in industries in which the skill level of the workers may be limited, and the best way to make sure that people will respond appropriately in an emergency is to provide a simple, clear set of instructions.
                
Note:Our flawed thinking in the checklist mentality that will fix our issues.

## Yellow highlight | Location: 9,376
ultimately, they still need to remain reliable, flexible, easy to manage in an emergency, well monitored, and capacity planned.
                

## Yellow highlight | Location: 9,378
“automate discovery, dashboard building, and alerting over a fleet of tens of thousands of machines.”
                

## Yellow highlight | Location: 9,399
An SRE team should be as compact as possible and operate at a high level of abstraction, relying upon lots of backup systems as failsafes and thoughtful APIs to communicate with the systems. At the same time, the SRE team should also have comprehensive knowledge of the systems — how they operate, how they fail, and how to respond to failures — that comes from operating them day-to-day.
                

## Yellow highlight | Location: 9,409
Availability table
                

## Yellow highlight | Location: 9,488
Postmortems Postmortems (see Chapter 15) should be blameless and focus on process and technology, not people. Assume the people involved in an incident are intelligent, are well intentioned, and were making the best choices they could given the information they had available at the time.
                

## Yellow highlight | Location: 9,518
SRE teams should spend no more than 50% of their time on operational work (see Chapter 5); operational overflow should be directed to the product development team.

